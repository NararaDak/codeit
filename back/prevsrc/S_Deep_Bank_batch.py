import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
# ğŸ’¡ ë°°ì¹˜ í•™ìŠµì„ ìœ„í•´ í•„ìš”í•œ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ì¶”ê°€
from torch.utils.data import TensorDataset, DataLoader 


# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'# Windows í•œê¸€ í°íŠ¸
plt.rcParams['axes.unicode_minus'] = False# ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.style.use('fivethirtyeight')

def Lines():
    print("â”€" * 120)

# ----------------------------------------------------------------------
# [ë°ì´í„° ë¡œë“œ, ì „ì²˜ë¦¬, ë¶„í•  ì½”ë“œ (ìƒëµ)]
# ----------------------------------------------------------------------
# (í¸ì˜ìƒ ì „ì²˜ë¦¬ ë° ë°ì´í„° ë¶„í•  ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  í•˜ë‹¨ë¶€ë§Œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.)
# ...
bank_data = pd.read_csv('D:/01.project/ì½”ë“œì‡/ë¯¸ì…˜/ë¯¸ì…˜4/data/bank-additional-full.csv' ,sep = ';')
bank_data_test = pd.read_csv('D:/01.project/ì½”ë“œì‡/ë¯¸ì…˜/ë¯¸ì…˜4/data/bank-additional.csv' ,sep = ';')
# ... [clean_dict, min_max_clean, age_by_5year_bands, preprocess_data í•¨ìˆ˜ ì •ì˜ ë° ì‹¤í–‰] ...
# ... [Feature ì •ì˜ ë° StandardScaler ì ìš© ì½”ë“œ (ìƒëµ)] ...



Lines()
print(bank_data.head())
Lines()
print(bank_data.info()) # ë°ì´í„° ì •ë³´ í™•ì¸
Lines()
# ê²°ì¸¡ì¹˜ í™•ì¸ : ê²°ì¸¡ì¹˜ëŠ” ì—†ëŠ” ê²ƒìœ¼ë¡œ ë‚˜ì˜´.
null_df = bank_data.isnull().sum().reset_index()
null_df.columns = ['ì»¬ëŸ¼ëª…', 'ê²°ì¸¡ì¹˜ ê°œìˆ˜']
print(null_df)
Lines()
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ì—­ ë§¤í•‘ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
clean_dict = {}  # ì „ì—­ ë§¤í•‘ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Min/Max ê¸°ì¤€ ê· ë“± ë„ˆë¹„ ë²”ì£¼í™” í•¨ìˆ˜
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
min_max_dict = {}

def min_max_clean(df, col_name, range_count=10, add_name="_clean"):
    """
    mode='fit': í›ˆë ¨ ë°ì´í„°ì˜ min/maxë¥¼ ê³„ì‚°í•˜ê³  ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    mode='transform': ë”•ì…”ë„ˆë¦¬ì— ì €ì¥ëœ min/maxë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    new_col_name = col_name + "_" + str(range_count) + add_name
     # 1. ìŠ¤ì¼€ì¼ëŸ¬ ì¤€ë¹„ ë° ëª¨ë“œ ì²´í¬
    if(new_col_name not in min_max_dict):
        # fit ëª¨ë“œ: MinMaxScalerë¥¼ ìƒˆë¡œ ìƒì„±í•˜ê³  í˜„ì¬ ë°ì´í„°ë¡œ í•™ìŠµ
        scaler = MinMaxScaler()
        scaler.fit(df[[col_name]])
        min_max_dict[new_col_name] = scaler  # í•™ìŠµëœ ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
        print(f"âœ… '{col_name}'ì˜ min/maxê°€ í•™ìŠµë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        # transform ëª¨ë“œ: ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì‚¬ìš©
        scaler = min_max_dict[new_col_name] # ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´ë¥¼ ì‚¬ìš©
    # 2. ì €ì¥ëœ min/max ê°’ê³¼ bin ë„ˆë¹„ ê³„ì‚° (fit ëª¨ë“œì—ì„œ ì´ë¯¸ í•™ìŠµë˜ì—ˆê±°ë‚˜, transform ëª¨ë“œì—ì„œ ë¡œë“œëœ ê°’)
    min_val = scaler.data_min_[0]
    max_val = scaler.data_max_[0]
    bin_width = (max_val - min_val) / range_count
    # max_val + 1e-5: ê²½ê³„ê°’ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìµœëŒ€ê°’ì— ì‘ì€ ê°’ì„ ë”í•¨
    bins = [min_val + i * bin_width for i in range(range_count)] + [max_val + 1e-5]

    # 3. pd.cutì„ ì‚¬ìš©í•˜ì—¬ ë²”ì£¼í™” (fit/transform ëª¨ë‘ ë™ì¼í•œ bins ì‚¬ìš©)
    cut_series = pd.cut( 
        df[col_name], 
        bins=bins, 
        include_lowest=True,
        duplicates='drop'
    )
    df[new_col_name] = cut_series.cat.codes
    return df

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‚˜ì´ ë²”ì£¼í™” í•¨ìˆ˜
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def age_by_5year_bands(df: pd.DataFrame, source_col: str, target_col: str):
    # Age ì»¬ëŸ¼ì„ 11ì„¸ ì´í•˜ (0), ì´í›„ 5ì„¸ ë‹¨ìœ„ë¡œ ë²”ì£¼í™”í•˜ê³  90ì„¸ ì´ìƒì„ í•˜ë‚˜ë¡œ ë¬¶ëŠ” í•¨ìˆ˜.
    # êµ¬ê°„ ê²½ê³„ ì„¤ì •
    bins = [1, 10] + list(range(15, 95, 5)) + [np.inf]
    
    # ê° êµ¬ê°„ì˜ ë ˆì´ë¸” (0,1,2,...)
    labels = list(range(len(bins) - 1))
    
    # ë²”ì£¼í™” ìˆ˜í–‰
    df[target_col] = pd.cut(
        df[source_col],
        bins=bins,
        labels=labels,
        right=True,
        include_lowest=True
    ).astype(int)
    
    # êµ¬ê°„ ë¬¸ìì—´ ì¶”ì¶œ
    intervals = pd.cut(
        df[source_col],
        bins=bins,
        right=True,
        include_lowest=True
    ).cat.categories.astype(str)
    
    # clean_dictì— (ìˆ«ì â†’ êµ¬ê°„) í˜•íƒœë¡œ ì €ì¥
    clean_dict[target_col] = {i: interval for i, interval in enumerate(intervals)}
    return df

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ 
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def preprocess_data(df):
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â–£ ë™ì¼í–‰ ì‚­ì œ â–£ 
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df =  df.drop_duplicates()
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â–£ pdays 999 íŠ¹ì´ê°’ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€ â–£ 
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. 'ì´ì „ ì ‘ì´‰ ì—¬ë¶€' í”Œë˜ê·¸ ìƒì„±: 999ê°€ ì•„ë‹ˆë©´ 1 (ì ‘ì´‰í–ˆìŒ), 999ë©´ 0 (ì ‘ì´‰ ì•ˆ í–ˆìŒ)
    df['pdays_contacted'] = np.where(df['pdays'] == 999, 0, 1)
    # 2. ì‹¤ì œ ê²½ê³¼ ì¼ìˆ˜ ì»¬ëŸ¼ ìƒì„±: 999ë¥¼ NaNìœ¼ë¡œ ëŒ€ì²´
    df['pdays_actual'] = df['pdays'].replace(999, np.nan)
    # 3. pdays_actualì˜ NaNì„ ìœ íš¨ê°’ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
    # (ì£¼ì˜: .median()ì€ NaNì„ ì œì™¸í•˜ê³  ê³„ì‚°í•©ë‹ˆë‹¤)
    median_pdays = df['pdays_actual'].median()
    df['pdays_actual'] = df['pdays_actual'].fillna(median_pdays)
    # 4. ìƒˆë¡œ ìƒì„±ëœ ì´ì§„ ì»¬ëŸ¼ì„ ë²”ì£¼í˜•ìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ clean_dictì— ë“±ë¡
    # ì´ì§„ ì»¬ëŸ¼ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ 0, 1ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, ë²”ì£¼í™”í•˜ì—¬ clean_dictì— ë“±ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ë²”ì£¼í™”í•˜ì—¬ clean_dictì— ë§¤í•‘ì„ ë‚¨ê¹ë‹ˆë‹¤.
    df['pdays_contacted_clean'] = df['pdays_contacted'].astype('category').cat.codes
    clean_dict['pdays_contacted_clean'] = {0: 'No Previous Contact (999)', 1: 'Had Previous Contact'}
    # 5. ìµœì¢… ì—°ì†í˜• ë³€ìˆ˜ì¸ pdays_actualì„ min_max_cleanì„ í†µí•´ ë²”ì£¼í™”
    min_max_clean(df, 'pdays_actual', range_count=10) # 5ê°œ êµ¬ê°„ìœ¼ë¡œ ë²”ì£¼í™” ì˜ˆì‹œ
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â–£ contact_freq_ratio ìƒì„± ë° ì²˜ë¦¬ â–£ 
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # previous / campaign ë¹„ìœ¨ ê³„ì‚°. campaignì€ ìµœì†Œ 1ì´ë¯€ë¡œ 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë¬¸ì œ ì—†ìŒ.
    df['contact_freq_ratio'] = df['previous'] / df['campaign']
    
    # ìƒì„±ëœ ë¹„ìœ¨ ë³€ìˆ˜ë¥¼ min_max_cleanì„ í†µí•´ ë²”ì£¼í™” (20ê°œ êµ¬ê°„)
    min_max_clean(df, 'contact_freq_ratio', range_count=20)
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â–£ recent_contact_flag ìƒì„± ë° ì¸ì½”ë”© â–£ 
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì¡°ê±´ì— ë”°ë¥¸ ê°’ í• ë‹¹
    conditions = [
        # ì¡°ê±´ 1: ì´ì „ ì ‘ì´‰ ì—†ìŒ (pdays=999)
        (df['pdays'] == 999), 
        # ì¡°ê±´ 2: ì´ì „ ì ‘ì´‰ ì„±ê³µ
        (df['pdays'] != 999) & (df['poutcome'] == 'success'),
        # ì¡°ê±´ 3: ì´ì „ ì ‘ì´‰ ì‹¤íŒ¨
        (df['pdays'] != 999) & (df['poutcome'] == 'failure')
    ]
    
    # í• ë‹¹í•  ë ˆì´ë¸”
    choices = ['NoContact', 'Success', 'Failure']
    
    # np.selectë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë²”ì£¼í˜• ì»¬ëŸ¼ ìƒì„±
    df['recent_contact_flag'] = np.select(conditions, choices, default='Other') # 'Other'ëŠ” ë°œìƒí•˜ì§€ ì•ŠìŒ
    
    # ë²”ì£¼í˜•ìœ¼ë¡œ ì¸ì½”ë”©í•˜ì—¬ clean_dictì— ë“±ë¡
    # ìˆœì„œ: NoContact(0), Failure(1), Success(2) (ì•ŒíŒŒë²³ ìˆœì„œëŒ€ë¡œ ì¸ì½”ë”©)
    df['recent_contact_flag_clean'] = df['recent_contact_flag'].astype('category').cat.codes
    
    # clean_dictì— ë§¤í•‘ ì €ì¥
    clean_dict['recent_contact_flag_clean'] = dict(enumerate(df['recent_contact_flag'].astype('category').cat.categories))
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â–£ ì¹´í…Œê³ ë¦¬ ë²”ì£¼ ìˆ«ìí˜• ì¸ì½”ë”© â–£ 
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    categorical_cols = [
        'job', 'marital', 'education', 'default', 'housing', 'loan', 
        'contact', 'month', 'day_of_week', 'poutcome'
    ]

    # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    for col in categorical_cols:
        if col == 'month':
            # 'month' íŠ¹ìˆ˜ ì²˜ë¦¬: 1ì›”ë¶€í„° 12ì›” ìˆœì„œëŒ€ë¡œ ì¸ì½”ë”© (Jan=0, Feb=1, ..., Dec=11)
            month_order = [
                'jan', 'feb', 'mar', 'apr', 'lender', # lender is a common value in bank marketing datasets,
                'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'
            ]
            # Lowercase all values for safety before setting categories
            df[col] = df[col].str.lower()
            
            # Ensure only relevant months are in the order, handling 'lender' if present,
            # or filtering to just the 12 calendar months if 'lender' isn't expected.
            present_months = df[col].unique().tolist()
            
            # Filter the month_order to only include months present in the data
            ordered_categories = [m for m in month_order if m in present_months]

            # Set the categorical type with explicit order
            df[col] = pd.Categorical(df[col], categories=ordered_categories, ordered=True)
            df[col + '_clean'] = df[col].cat.codes
            
            # ìˆ«ì â†’ ë¬¸ìì—´ í˜•íƒœë¡œ clean_dict ì €ì¥
            # cat.categories will now be in the specified order (jan, feb, ...)
            clean_dict[col + '_clean'] = dict(enumerate(df[col].cat.categories))
        elif col == 'day_of_week':
            # 'day_of_week' íŠ¹ìˆ˜ ì²˜ë¦¬: ì›”ìš”ì¼ë¶€í„° ì¼ìš”ì¼ ìˆœì„œëŒ€ë¡œ ì¸ì½”ë”© (mon=0, tue=1, ..., sun=6)
            day_order = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']
            df[col] = df[col].str.lower()
            df[col] = pd.Categorical(df[col], categories=day_order, ordered=True)
            df[col + '_clean'] = df[col].cat.codes
            # ìˆ«ì â†’ ë¬¸ìì—´ í˜•íƒœë¡œ clean_dict ì €ì¥
            clean_dict[col + '_clean'] = dict(enumerate(df[col].cat.categories))
        else:
            # ê¸°ë³¸ ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
            df[col] = df[col].astype('category')
            df[col + '_clean'] = df[col].cat.codes
            
            # ìˆ«ì â†’ ë¬¸ìì—´ í˜•íƒœë¡œ clean_dict ì €ì¥
            clean_dict[col + '_clean'] = dict(enumerate(df[col].cat.categories))

    # íƒ€ê²Ÿ ë³€ìˆ˜ ë³€í™˜ ë° clean_dict ë“±ë¡
    target_map = {'no': 0, 'yes': 1}
    # Ensure target variable 'y' is consistent (e.g., lowercase)
    # Assuming 'y' is in the format 'yes'/'no'
    if 'y' in df.columns:
        df['y_clean'] = df['y'].astype(str).str.lower().map(target_map)
        clean_dict['y_clean'] = {v: k for k, v in target_map.items()} # {0:'no', 1:'yes'}
    else:
        # Handle case where 'y' might be missing in the input (e.g., prediction phase)
        pass 
    #------------------------------------------------------------------
    # 'job'ë³„ í‰ê·  ë‚˜ì´ ê³„ì‚°
    job_age_mean = df.groupby('job')['age'].transform('mean')
    #  ê°œì¸ ë‚˜ì´ì™€ ì§ì—… í‰ê·  ë‚˜ì´ì˜ ì°¨ì´ ê³„ì‚°
    df['job_age_mean_diff'] = df['age'] - job_age_mean
    # ìƒˆë¡œìš´ íŒŒìƒ ë³€ìˆ˜ì— ëŒ€í•´ min_max_clean ì ìš© (ë²”ì£¼í™”)
    # 20ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ ì„œ ë””í…Œì¼í•œ ì°¨ì´ë¥¼ ë°˜ì˜
    min_max_clean(df, 'job_age_mean_diff', range_count=20)
    #------------------------------------------------------------------
    # ë‚˜ì´ ë²”ì£¼í™” (clean_dictì— ìë™ ë“±ë¡ë¨)
    age_by_5year_bands(df, 'age', 'age_clean')
    #min_max_clean(df, 'duration')
    #min_max_clean(df, 'campaign')
    min_max_clean(df, 'duration', 100)
    #min_max_clean(df, 'campaign', 100)
    min_max_clean(df, 'euribor3m', 100)
    min_max_clean(df, 'nr.employed', 10)
    min_max_clean(df, 'emp.var.rate',20)
    min_max_clean(df, 'cons.price.idx',20)
    min_max_clean(df, 'cons.conf.idx',20)
    return df

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰ 
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bank_data = preprocess_data(bank_data)
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bank_data_test = preprocess_data(bank_data_test)
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# StandardScaler  
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# ë°ì´í„° ë¶„í• 
# x_data, y_data ì •ì˜ (ì „ì²˜ë¦¬ëœ bank_data ì‚¬ìš©)
feature = [
"age_clean", "job_clean", "marital_clean", "education_clean", "default_clean", 
"housing_clean", "loan_clean", "contact_clean", "month_clean", "day_of_week_clean", 
"duration_100_clean", "campaign", "previous", "poutcome_clean", 
"cons.price.idx_20_clean", "cons.conf.idx_20_clean", "euribor3m_100_clean", 
"nr.employed_10_clean", "emp.var.rate_20_clean", "pdays_contacted", 
"pdays_actual_10_clean",
]
label = ['y_clean']

# StandardScaler ì ìš© (fit_transformì€ ì´ë¯¸ ìˆ˜í–‰ë˜ì—ˆë‹¤ê³  ê°€ì •)
# bank_data[feature] = std.fit_transform(bank_data[feature]) ...

x_data = bank_data[feature]
y_data = bank_data[label]
x_train,x_val,y_train,y_val = train_test_split(x_data,y_data,test_size=0.2,random_state=42)
x_test = bank_data_test[feature]
y_test = bank_data_test[label]

# ëª¨ë¸ í•™ìŠµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. í•˜ì´í¼ íŒŒë¼ë©”í„° ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LEARNING_RATE = 0.01
EPOCHS = 10000
H1 = len(feature)
H2 = H1
FEATURE_COUNT = H1
BATCH_SIZE = 500
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ëª¨ë¸ ì •ì˜ (ì€ë‹‰ì¸µ 1ê°œ, ReLU í™œì„±í™”, ì¶œë ¥ì¸µ Sigmoid)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def makeModel():
    model = nn.Sequential(
        nn.Linear(len(feature), H1),
        nn.ReLU(),
        nn.Linear(H1, H2),
        nn.ReLU(),
        nn.Linear(H2, 1),
        nn.Sigmoid()
    )
    return model

model = makeModel()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. í•™ìŠµ ë£¨í”„ ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. í…ì„œë¥¼ ìƒì„±í•˜ê³  ì¥ì¹˜ë¡œ ì´ë™ (BCELossë¥¼ ìœ„í•´ y_tensorëŠ” (N, 1) í˜•íƒœ)
X_tensor = torch.tensor(x_train.values, dtype=torch.float32)
y_tensor = torch.tensor(y_train.values, dtype=torch.float32)

# 3. TensorDataset ë° DataLoader ìƒì„± ğŸ’¡ (ë°°ì¹˜ í•™ìŠµì˜ í•µì‹¬)
train_dataset = TensorDataset(X_tensor, y_tensor)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

# 4. í•™ìŠµ ë£¨í”„ (ë°°ì¹˜ í•™ìŠµ)
def train():
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    for epoch in range(EPOCHS):
        model.train()
        total_epoch_loss = 0.0
        
        # ğŸ’¡ DataLoaderë¥¼ ìˆœíšŒí•˜ë©° ë¯¸ë‹ˆ ë°°ì¹˜ í•™ìŠµ ìˆ˜í–‰
        for X_batch, y_batch in train_loader:
            # X_batchì™€ y_batchëŠ” ì´ë¯¸ deviceë¡œ ì´ë™ë˜ì–´ ìˆìŒ

            
            # ìˆœì „íŒŒ
            outputs = model(X_batch)
            
            # ì†ì‹¤ê³„ì‚°
            loss = criterion(outputs, y_batch)
            
            optimizer.zero_grad() # ê° ë°°ì¹˜ë§ˆë‹¤ ê¸°ìš¸ê¸° ì´ˆê¸°í™”!
            # ì—­ì „íŒŒ ë° ìµœì í™”
            loss.backward() # ì—­ì „íŒŒë¥¼ í†µí•´ ê¸°ìš¸ê¸° ê³„ì‚°.
            optimizer.step() # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸.
            
            # ë°°ì¹˜ ì†ì‹¤ ëˆ„ì  (ì •í™•í•œ ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ë°˜ì˜)
            total_epoch_loss += loss.item() * X_batch.size(0)

        # ì—í­ í‰ê·  ì†ì‹¤ ê³„ì‚°
        avg_epoch_loss = total_epoch_loss / len(train_dataset)
        
        if epoch % 1000 == 0:
            print(f"Epoch {epoch:05d} | Avg. Loss: {avg_epoch_loss:.6f}")
            
        if avg_epoch_loss <= 0.00001:
            print(f"í•™ìŠµ ì¢…ë£Œ: Epoch {epoch:05d} | Avg. Loss: {avg_epoch_loss:.6f}")
            break

# í•™ìŠµ ì‹¤í–‰
train()

# í‰ê°€
from sklearn.metrics import accuracy_score
model.eval()

with torch.no_grad():
    # í‰ê°€ ì‹œ ì „ì²´ X_tensorë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì´ë¯¸ deviceì— ìˆìŒ)
    outputs = model(X_tensor)
    
    # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ NumPyë¡œ ë³€í™˜
    
    predicted = (outputs.numpy() > 0.5).astype(float)
    # y_trainì€ ë°ì´í„°í”„ë ˆì„ì´ ì•„ë‹Œ í…ì„œì˜ CPU ë²„ì „(y_train_cpu)ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    accuracy = accuracy_score(y_train, predicted) 
    print(f"í›ˆë ¨ ì„¸íŠ¸ ì •í™•ë„: {accuracy:.4f}")