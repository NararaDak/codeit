{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18460,
     "status": "ok",
     "timestamp": 1740465988334,
     "user": {
      "displayName": "Harper Lee",
      "userId": "11182007330807379238"
     },
     "user_tz": -540
    },
    "id": "OGkhBpIJ2cAT",
    "outputId": "7985ecba-3099-4858-cae1-bfeb902811c9"
   },
   "outputs": [],
   "source": [
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1740465991130,
     "user": {
      "displayName": "Harper Lee",
      "userId": "11182007330807379238"
     },
     "user_tz": -540
    },
    "id": "RE-iiDw-2yV3",
    "outputId": "6d50ce0f-3647-4433-f7c0-46eef701dff4"
   },
   "outputs": [],
   "source": [
    "!cd data && unzip PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 174866,
     "status": "ok",
     "timestamp": 1740466177768,
     "user": {
      "displayName": "Harper Lee",
      "userId": "11182007330807379238"
     },
     "user_tz": -540
    },
    "id": "lK9POTQDyrhV",
    "outputId": "5ecfd177-f47f-4024-aab7-2fe0e02c34ba"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "BASE_DIR = r\"D:\\01.project\\antig\\mission8\\data\\PennFudanPed\"\n",
    "\n",
    "# ======================\n",
    "# PennFudanPed Dataset\n",
    "# ======================\n",
    "class PennFudanDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\"\n",
    "        root: PennFudanPed 폴더의 상위 경로 (예: \"./data/PennFudanPed\")\n",
    "        transform: 이미지 및 마스크에 적용할 transform (동일하게 적용)\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.imgs_dir = os.path.join(root, \"PNGImages\")\n",
    "        self.masks_dir = os.path.join(root, \"PedMasks\")\n",
    "        self.imgs = list(sorted(os.listdir(self.imgs_dir)))\n",
    "        self.masks = list(sorted(os.listdir(self.masks_dir)))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지 로드\n",
    "        img_path = os.path.join(self.imgs_dir, self.imgs[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        # mask는 각 인스턴스마다 다른 값이지만, 이진 분할을 위해\n",
    "        # 0은 배경, 그 외는 보행자(1)로 처리\n",
    "        mask = np.array(mask)\n",
    "        # 여러 인스턴스 픽셀값 > 0을 1로 변경\n",
    "        mask = np.where(mask > 0, 1, 0).astype('uint8')\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        # transform 적용 (동일하게 이미지와 마스크에 적용)\n",
    "        if self.transform is not None:\n",
    "            # mask의 경우 PIL.Image.NEAREST를 사용해야 함\n",
    "            img, mask = self.transform(img, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "# 예시: transform 함수 (resize, tensor 변환)\n",
    "def joint_transform(img, mask, size=(256, 256)):\n",
    "    # Resize: 이미지와 mask에 동일하게 적용\n",
    "    img = TF.resize(img, size)\n",
    "    mask = TF.resize(mask, size, interpolation=Image.NEAREST)\n",
    "    # ToTensor: 이미지는 [0,1] 실수 tensor, mask는 int tensor\n",
    "    img = TF.to_tensor(img)\n",
    "    mask = torch.as_tensor(np.array(mask), dtype=torch.long)\n",
    "    return img, mask\n",
    "\n",
    "# ======================\n",
    "# UNet 모델 정의\n",
    "# ======================\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=2):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(64, 128)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(128, 256)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(256, 512)\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(512, 512)\n",
    "        )\n",
    "        self.up1 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
    "        self.conv1 = DoubleConv(1024, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(512, 128)\n",
    "        self.up3 = nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(256, 64)\n",
    "        self.up4 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)     # [B,64,H,W]\n",
    "        x2 = self.down1(x1)  # [B,128,H/2,W/2]\n",
    "        x3 = self.down2(x2)  # [B,256,H/4,W/4]\n",
    "        x4 = self.down3(x3)  # [B,512,H/8,W/8]\n",
    "        x5 = self.down4(x4)  # [B,512,H/16,W/16]\n",
    "\n",
    "        x = self.up1(x5)     # upsample to H/8\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.up2(x)      # H/4\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.up3(x)      # H/2\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = self.up4(x)      # H\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 학습 및 평가 루프\n",
    "# ======================\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, masks in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        # masks: [B, H, W] -> loss 함수(CrossEntropyLoss)는 class index를 기대하므로 shape 그대로 사용\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)  # outputs: [B, n_classes, H, W]\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 179363,
     "status": "ok",
     "timestamp": 1740467113524,
     "user": {
      "displayName": "Harper Lee",
      "userId": "11182007330807379238"
     },
     "user_tz": -540
    },
    "id": "2z_CGZHv6afE",
    "outputId": "66a7111b-35e6-473a-fea9-bf0ea4863386"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 메인 실행 코드\n",
    "# ======================\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "num_epochs = 25\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset 및 DataLoader\n",
    "dataset = PennFudanDataset(root=BASE_DIR, transform=joint_transform)\n",
    "# 학습/검증 분할 (예: 80% train, 20% val)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "asize=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# 모델, 손실함수, optimizer\n",
    "model = UNet(n_channels=3, n_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # output: [B, 2, H, W], mask: [B, H, W] (각 픽셀 0 또는 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 학습 루프\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    # 가장 낮은 검증 손실 모델 저장 (옵션)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_unet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cwshyx0h2amJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_predictions(model, dataloader, device, num_images=4):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)  # 출력: [B, n_classes, H, W]\n",
    "            # 예측: 각 픽셀에서 채널의 argmax (배경:0, 보행자:1)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            imgs = imgs.cpu().numpy().transpose(0, 2, 3, 1)  # [B, H, W, C]로 변경\n",
    "            masks = masks.cpu().numpy()\n",
    "\n",
    "            for i in range(min(num_images, imgs.shape[0])):\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                axs[0].imshow(imgs[i])\n",
    "                axs[0].set_title(\"Input Image\")\n",
    "                axs[0].axis(\"off\")\n",
    "                axs[1].imshow(masks[i], cmap=\"gray\")\n",
    "                axs[1].set_title(\"Ground Truth\")\n",
    "                axs[1].axis(\"off\")\n",
    "                axs[2].imshow(preds[i], cmap=\"gray\")\n",
    "                axs[2].set_title(\"Predicted Mask\")\n",
    "                axs[2].axis(\"off\")\n",
    "                plt.show()\n",
    "            break\n",
    "\n",
    "val_dataloader_shuffle = DataLoader(val_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1621,
     "status": "ok",
     "timestamp": 1740467810527,
     "user": {
      "displayName": "Harper Lee",
      "userId": "11182007330807379238"
     },
     "user_tz": -540
    },
    "id": "TtkApOAU6Nqw",
    "outputId": "af7c5a18-bc45-45fd-e2dc-5fb8ff3a5494"
   },
   "outputs": [],
   "source": [
    "visualize_predictions(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCtdbmke6PfC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
