import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.transforms import v2
import cv2
from transformers import AutoModel,AutoImageProcessor
import os
import torchvision.transforms.functional as TF
import torch.nn.functional as  F
from torch.utils.data import Dataset, DataLoader
from PIL import Image, ImageEnhance, ImageFilter
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import numpy as np

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def Lines(text = "", count = 100):
    print("â”€"*100)
    if text != "":
        print(f"{text}")
        print("â”€"*count)

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë””ë ‰í† ë¦¬ ì§€ì •. 
base_dir = "/home/nabi/project/ai/mission/mission5/AI_DATA/Mission5_Data"
base_dir = "D:/AI_DATA/Mission5_Data"
train_dir = f"{base_dir}/train" # Train data X.
train_cleaned_dir = f"{base_dir}/train_cleaned" # Train data Y.(Label)
test_dir = f"{base_dir}/test" # Test data X.
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³€í™˜ í´ë˜ìŠ¤. 
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class PadToSquare(nn.Module):
    def __init__(self, size: int, fill: float = 0.0):
        super().__init__()
        self.size = size
        self.fill = fill
    
    def forward_org(self, img: torch.Tensor) -> torch.Tensor:
        # í…ì„œëŠ” (C, H, W) í˜•íƒœì—¬ì•¼ í•¨
        if img.ndim != 3:
            raise ValueError("Input must be a 3D Tensor (C, H, W).")
            
        _, h, w = img.shape
        
        # H, Wê°€ íƒ€ê²Ÿ í¬ê¸°ë³´ë‹¤ ì‘ì„ ê²½ìš°ì—ë§Œ íŒ¨ë”© ê³„ì‚°
        h_pad = self.size - h
        w_pad = self.size - w
        
        # íŒ¨ë”©ì€ ì´ë¯¸ì§€ì˜ ì¤‘ì•™ì— ì˜¤ë„ë¡ ìœ„/ì•„ë˜, ì¢Œ/ìš°ì— ê· ë“±í•˜ê²Œ ë¶„ë°°
        padding_left = w_pad // 2
        padding_right = w_pad - padding_left
        padding_top = h_pad // 2
        padding_bottom = h_pad - padding_top
        
        # í…ì„œì— íŒ¨ë”© ì ìš© (ìˆœì„œ: left, top, right, bottom)
        # fill ê°’ì€ í…ì„œì˜ dtypeì— ë§ì¶° floatìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
        return F.pad(img, 
                     [padding_left, padding_top, padding_right, padding_bottom],
                     padding_mode='constant',
                     fill=self.fill)
    def forward(self, img: torch.Tensor) -> torch.Tensor:
        # í…ì„œëŠ” (C, H, W) í˜•íƒœì—¬ì•¼ í•¨
        if img.ndim != 3:
            raise ValueError("Input must be a 3D Tensor (C, H, W).")
            
        _, h, w = img.shape
        
        # H, Wê°€ íƒ€ê²Ÿ í¬ê¸°ë³´ë‹¤ ì‘ì„ ê²½ìš°ì—ë§Œ íŒ¨ë”© ê³„ì‚° (ì—¬ê¸°ì„œëŠ” íƒ€ê²Ÿ í¬ê¸°ê°€ self.sizeë¼ê³  ê°€ì •)
        h_pad = max(0, self.size - h)
        w_pad = max(0, self.size - w)
        
        # íŒ¨ë”©ì€ ì´ë¯¸ì§€ì˜ ì¤‘ì•™ì— ì˜¤ë„ë¡ ìœ„/ì•„ë˜, ì¢Œ/ìš°ì— ê· ë“±í•˜ê²Œ ë¶„ë°°
        padding_left = w_pad // 2
        padding_right = w_pad - padding_left
        padding_top = h_pad // 2
        padding_bottom = h_pad - padding_top
        
        # í…ì„œì— íŒ¨ë”© ì ìš© (ìˆœì„œ: W_left, W_right, H_top, H_bottom)
        # WëŠ” ê°€ì¥ ì•ˆìª½ ì°¨ì›, HëŠ” ê·¸ ë‹¤ìŒ ì°¨ì›ì´ë¯€ë¡œ ì´ ìˆœì„œê°€ ë§ìŒ
        return F.pad(img, 
                     [padding_left, padding_right, padding_top, padding_bottom],
                     mode='constant', # 'padding_mode'ë¥¼ 'mode'ë¡œ ë³€ê²½
                     value=self.fill) # 'fill'ì„ 'value'ë¡œ ë³€ê²½
        
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë³€ê²½ ê°ì²´
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TARGET_SIZE = 224
g_transforms_org = v2.Compose(
    [
        v2.ToImage(),
        v2.Grayscale(num_output_channels=1), 
        # 1. ë¬¸ì„œ ì „ì²´ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©° ëª¨ë¸ ì…ë ¥ í¬ê¸°(224x224)ì— ë§ê²Œ ì¶•ì†Œ
        v2.Resize(224, antialias=True), # 420x540ì´ 224x224ë³´ë‹¤ í¬ë¯€ë¡œ ì¶•ì†Œë¨. ê°€ì¥ ì§§ì€ ë³€ì„ 224ì— ë§ì¶¤.
        # 2. íŒ¨ë”© ëŒ€ì‹ , ì¶•ì†Œëœ ì´ë¯¸ì§€ë¥¼ 224x224 í¬ê¸°ì— ë§ê²Œ íŒ¨ë”© (í•„ìš”í•˜ë‹¤ë©´)
        PadToSquare(TARGET_SIZE, fill=1.0),
        # 3. ëª¨ë¸ ì…ë ¥ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ ë° ì •ê·œí™”
        v2.ToDtype(dtype=torch.float32, scale=True)
    ]
)

g_transforms = transforms.Compose([
    # 1. Resize: ì§§ì€ ì¶•ì„ TARGET_SIZE(224) ì´ìƒìœ¼ë¡œ ì¡°ì • (í•„ìˆ˜ ì „ì²˜ë¦¬)
    # ì´ë¯¸ì§€ì˜ ì¢…íš¡ë¹„ë¥¼ ìœ ì§€í•˜ë©´ì„œ í¬ê¸°ë¥¼ í‚¤ì›Œ, ë‹¤ìŒ ë‹¨ê³„ì¸ Cropì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    transforms.Resize(TARGET_SIZE, antialias=True), 
    
    # 2. Crop: ì´ë¯¸ì§€ë¥¼ TARGET_SIZE x TARGET_SIZEë¡œ ê°•ì œì ìœ¼ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤. (í¬ê¸° í†µì¼)
    # ì´ ë‹¨ê³„ë¥¼ í†µí•´ ëª¨ë“  ì´ë¯¸ì§€ì˜ [H, W]ê°€ [224, 224]ë¡œ ê³ ì •ë©ë‹ˆë‹¤.
    transforms.CenterCrop(TARGET_SIZE), 
    
    # 3. ToTensor: PIL ì´ë¯¸ì§€ë¥¼ í…ì„œ [C, H, W]ë¡œ ë³€í™˜ (í•„ìˆ˜)
    transforms.ToTensor(),
    
    # 4. Normalize: (ì„ íƒì ) í”½ì…€ ê°’ ì •ê·œí™”
    # DINOv2 ì‚¬ì „ í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ì •ê·œí™” ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸° í•¨ìˆ˜
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        if filename.endswith(".png"):  # PNG íŒŒì¼ë§Œ ê°€ì ¸ì˜¤ê¸°
            img_path = os.path.join(folder, filename)
            img = cv2.imread(img_path)  # OpenCVë¡œ ì´ë¯¸ì§€ë¥¼ ì½ìŒ (BGR í˜•ì‹)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGR â†’ RGB ë³€í™˜
            images.append(img)
    return images
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  X-Y(Label) ë°ì´í„°ë¥¼ ë§Œë“ ë‹¤.(ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ì™€ í´ë¦°(ì •ë‹µ)ì´ë¯¸ì§€ ìƒì„±).
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì´ë¯¸ì§€ ê°€ì ¸ ì˜¤ê¸° í•¨ìˆ˜.
class loadImageDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_files = sorted( [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.png')])
        self.transform = transform
    def __len__(self):
        return len(self.data_files)
    def __getitem__(self, idx):
        img_path = self.data_files[idx]
        image = Image.open(img_path).convert('RGB')
        # ë³€í™˜ ì ìš©
        if self.transform:
            image = self.transform(image)
        return image
# Paired - ì´ë¯¸ì§€(X,y(label) ë§Œë“¤ê¸°)
class PairedImageDataset(Dataset):
    def __init__(self, train_dir, train_cleaned_dir, transform=None):
        """
        ë‘ ê°œì˜ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê³  ë§¤ì¹­.
        :param train_dir: ì›ë³¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
        :param train_cleaned_dir: ì •ì œëœ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
        :param transform: ì›ë³¸ ì´ë¯¸ì§€ì— ì ìš©í•  ë°ì´í„° ë³€í™˜
        :param transform_cleaned: ì •ì œëœ ì´ë¯¸ì§€ì— ì ìš©í•  ë°ì´í„° ë³€í™˜
        """
        self.train_image = loadImageDataset(train_dir, transform)
        self.cleaned_image = loadImageDataset(train_cleaned_dir, transform)
        assert len(self.train_image) == len(self.cleaned_image), "trainê³¼ cleaned ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ ë‹¤ë¦…ë‹ˆë‹¤."
        self.transform = transform
        self.train_len = len(self.train_image)
    def __len__(self):
        return self.train_len
    def __getitem__ort(self, idx):
        return self.train_image, self.cleaned_image
    def __getitem__(self, idx):
        train_img = self.train_image[idx]  # loadImageDataset.__getitem__ í˜¸ì¶œ (í…ì„œ ë°˜í™˜ ì˜ˆìƒ)
        cleaned_img = self.cleaned_image[idx]
        return train_img, cleaned_img
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™•ì¸ìš© í•¨ìˆ˜ 
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def view_paired_dataset(pairedLoader, numImages=5):
    # paired_loaderì—ì„œ ë°ì´í„° í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    for trainImages, cleanedImages in pairedLoader:
        # ì‹œê°í™”
        fig, axes = plt.subplots(numImages, 2, figsize=(8, numImages * 3))  # num_images í–‰, 2ì—´ (ìœ„: train, ì•„ë˜: cleaned)
        for i in range(numImages):
            # ì²« ë²ˆì§¸ ì—´: ì›ë³¸ ì´ë¯¸ì§€ (train)
            axes[i, 0].imshow(trainImages[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')  # [C, H, W] -> [H, W, C]
            axes[i, 0].set_title(f"Original {i+1}")
            axes[i, 0].axis('off')  # ì¶• ë¹„í™œì„±í™”

            # ë‘ ë²ˆì§¸ ì—´: ì •ì œëœ ì´ë¯¸ì§€ (cleaned)
            axes[i, 1].imshow(cleanedImages[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')  # [C, H, W] -> [H, W, C]
            axes[i, 1].set_title(f"Cleaned {i+1}")
            axes[i, 1].axis('off')  # ì¶• ë¹„í™œì„±í™”

        plt.tight_layout()
        plt.show(block = False)
        plt.pause(3)
        plt.close()
        break  # í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ë„ë¡ break
##â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# PairedImageDatasetì—ì„œ ì´ë¯¸ì§€ ì‹œê°í™”
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ëª¨ë¸ ê°€ì ¸ ì˜¤ê¸°. 
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ëª¨ë¸ ì •ì˜ í•¨ìˆ˜ (GetExtractAutoMode) ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ì—ì„œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
import torch.nn as nn
# from transformers import AutoModel, AutoImageProcessor # ì´ë¯¸ ì„í¬íŠ¸ë˜ì—ˆë‹¤ê³  ê°€ì •

class DenoisingModel(nn.Module):
    def __init__(self, pretrained_backbone, target_image_size=224):
        super().__init__()
        self.backbone = pretrained_backbone
        
        # DINOv2-baseì˜ íŠ¹ì§• í¬ê¸°ëŠ” 768ì…ë‹ˆë‹¤.
        # Vision Transformerì˜ ì¶œë ¥ì€ [B, N_patches, 768] í˜•íƒœì…ë‹ˆë‹¤.
        # ë³µì›ì„ ìœ„í•´ íŠ¹ì§•ì„ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•œ 2D í˜•íƒœë¡œ ë³€í™˜í•˜ê³  ì—…ìƒ˜í”Œë§í•´ì•¼ í•©ë‹ˆë‹¤.
        
        # ì„ì‹œ ë³µì› í—¤ë“œ: 768ì°¨ì› íŠ¹ì§•ì„ ë°›ì•„ 3ì±„ë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ë° ì—…ìƒ˜í”Œë§
        # âš ï¸ ì´ ë¶€ë¶„ì€ DINOv2ì˜ íŒ¨ì¹˜ í¬ê¸°ì™€ ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¼ ë³µì¡í•œ Reshape ë° Transpose ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
        # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ConvTranspose ë ˆì´ì–´ ì‹œí€€ìŠ¤ë¡œ ì˜ˆì‹œë¥¼ ë“­ë‹ˆë‹¤.
        self.restoration_head = nn.Sequential(
           # 1. 16x16 -> 32x32 (ì…ë ¥ 768)
            nn.ConvTranspose2d(768, 256, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            # 2. 32x32 -> 64x64
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            # 3. 64x64 -> 128x128
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            
            # ğŸŸ¢ ë§ˆì§€ë§‰ ì¸µ: 128x128 -> 224x224ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
            # ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•ì€ Stride=1, Kernel=1ì„ ì‚¬ìš©í•˜ì—¬ ì±„ë„ë§Œ ì¡°ì •í•œ í›„, 
            # F.interpolateë¥¼ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” ConvTransposeë¡œë§Œ í•´ê²°í•˜ê² ìŠµë‹ˆë‹¤.
            
            # 4. 128x128 -> 224x224 (ì¶œë ¥ í¬ê¸° 224ë¥¼ ê°•ì œí•˜ëŠ” ë³µì¡í•œ ì„¤ì •)
            # 128ì—ì„œ 224ê°€ ë˜ë ¤ë©´ í™•ì¥ ì¸ìê°€ 1.75ì—¬ì•¼ í•©ë‹ˆë‹¤. 
            # ConvTransposeë¡œ 1.75ë°° í™•ì¥ì€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, 
            # ë§ˆì§€ë§‰ ë ˆì´ì–´ ì „ì— í¬ê¸°ë¥¼ $112 \times 112$ë¡œ ë‚®ì¶°ì•¼ í•©ë‹ˆë‹¤.
            
            # 3ë²ˆ ì¸µì˜ ì¶œë ¥ì„ 112ë¡œ ê°•ì œí•˜ëŠ” ê²ƒì´ ë” ì‰½ìŠµë‹ˆë‹¤.
            # 64x64 -> 112x112: K=3, S=2, P=1, OP=0 (Output = 2*64 - 2*1 + 3 = 129 -> ì•ˆë¨)
            
            # 128x128ì—ì„œ 224x224ë¡œ ê°€ëŠ” ConvTranspose (Stride=1, Kernel=1ì„ ì œì™¸í•œ)ëŠ” ë„ˆë¬´ ë³µì¡í•©ë‹ˆë‹¤.
            
            # ğŸŒŸ ëŒ€ì•ˆ: 128x128ì—ì„œ Stride 1ì˜ Convë¥¼ í†µê³¼ì‹œí‚¨ í›„, F.interpolateë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ğŸŒŸ
            nn.Conv2d(64, 3, kernel_size=3, padding=1), # ì±„ë„ë§Œ 3ìœ¼ë¡œ ì¡°ì • (í¬ê¸° 128x128 ìœ ì§€)
            nn.Upsample(size=(target_image_size, target_image_size), mode='bilinear', align_corners=False), # 224x224ë¡œ ê°•ì œ ì¡°ì •
            #nn.Sigmoid()
        )
        
        self.target_size = target_image_size
        
        # ğŸŸ¢ ìˆ˜ì •ëœ ë¶€ë¶„: patch_sizeë¥¼ num_patches ê³„ì‚° ì „ì— ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
        self.patch_size = 16 # DINOv2-baseì˜ ê¸°ë³¸ íŒ¨ì¹˜ í¬ê¸°
        # íŒ¨ì¹˜ í† í° ìˆ˜ì—ì„œ í´ë˜ìŠ¤ í† í° ì œì™¸ ë“±ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì„ì‹œ ì°¨ì› ë³€í™˜ê¸°
        self.target_size = target_image_size
        self.num_patches = (self.target_size // self.patch_size) ** 2

    def forward(self, x):
        # 1. íŠ¹ì§• ì¶”ì¶œ: outputs.last_hidden_stateëŠ” [B, N_patches+1, 768] (í´ë˜ìŠ¤ í† í° í¬í•¨)
        features = self.backbone(x).last_hidden_state
        
        # 2. í´ë˜ìŠ¤ í† í° ì œì™¸ (N_patches = 196ì„ ì–»ê¸° ìœ„í•´)
        patch_features = features[:, 1:, :] # [B, 196, 768]
        
        # 3. 1D íŠ¹ì§•ì„ 2D ê·¸ë¦¬ë“œë¡œ Reshape
        B, N, C = patch_features.shape
        H = W = int(N**0.5) # H=W=14
        
        # [B, N_patches, 768] -> [B, 768, 14, 14]
        patch_features = patch_features.permute(0, 2, 1).view(B, C, H, W)
        
        # 4. ë³µì› í—¤ë“œë¥¼ í†µê³¼ (14x14 -> 224x224)
        output_image = self.restoration_head(patch_features)
        return torch.sigmoid(output_image)

def GetExtractAutoMode():
    modelName = "facebook/dinov2-base"
    image_processor = AutoImageProcessor.from_pretrained(modelName)
    Lines()
    print(f"feature:{image_processor}")
    Lines()
    pretrainedModel = AutoModel.from_pretrained(modelName)
    return DenoisingModel(pretrainedModel)

#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¶”ê°€ í›ˆë ¨ í•¨ìˆ˜. 
#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
g_Device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # ì¥ì¹˜ ì„¤ì •
def ExtraTrain(pairedLoader, epochs, lr):
   
    pretrainedModel = GetExtractAutoMode().to(g_Device) # ëª¨ë¸ì„ ì¥ì¹˜ë¡œ ì´ë™
    # 1. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (MSE Loss)
    loss_fn = nn.MSELoss() 
    # 2. ì˜µí‹°ë§ˆì´ì € ì •ì˜ (Adam)
    optimizer = optim.Adam(pretrainedModel.parameters(), lr=lr)
    pretrainedModel.train()
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸŒŸ ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€ ğŸŒŸ
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # DataLoaderì—ì„œ ì²« ë²ˆì§¸ ë°°ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    with torch.no_grad():
        for trainImages_test, cleanImages_test in pairedLoader:
            trainImages_test = trainImages_test.to(g_Device)
            outputs_test = pretrainedModel(trainImages_test)
            Lines("DEBUG: OUTPUT SHAPE CHECK")
            print(f"Model Output Shape (outputs): {outputs_test.shape}")
            print(f"Target Image Shape (cleanedImages, expected): {trainImages_test.shape}")
            Lines()
            
            cleanImages_test = cleanImages_test.to(g_Device)
            outputs_test = pretrainedModel(cleanImages_test)
            Lines("DEBUG: OUTPUT SHAPE CHECK")
            print(f"Model Output Shape (outputs): {outputs_test.shape}")
            print(f"Cleaned Image Shape (cleanedImages, expected): {cleanImages_test.shape}")
            Lines()
            break # ì²« ë°°ì¹˜ë§Œ í™•ì¸
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸŒŸ ë””ë²„ê¹… ì½”ë“œ ì¢…ë£Œ ğŸŒŸ
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 
    # í•™ìŠµ ì‹œì‘
    for epoch in range(epochs):
        total_loss = 0
        # for trainImages, cleanedImages in tqdm(pairedDataset, desc=f"Epoch {epoch+1}"): # DataLoaderê°€ ì•„ë‹Œ Datasetì„ ì§ì ‘ ìˆœíšŒ
        index = 0
        for trainImages, cleanedImages in pairedLoader:
            # ë°ì´í„°ë¥¼ ì¥ì¹˜ë¡œ ì´ë™
            trainImages = trainImages.to(g_Device)
            cleanedImages = cleanedImages.to(g_Device)

            # ìˆœì „íŒŒ
            outputs = pretrainedModel(trainImages)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = loss_fn(outputs, cleanedImages)
            
            # ì—­ì „íŒŒ
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            print(f"[{index}/{len(pairedLoader)}], Loss: {loss.item():.4f}", end='\r')
            index += 1

        avg_loss = total_loss / len(pairedLoader)
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}")

    print("Fine-tuning complete.")
    return pretrainedModel # í›ˆë ¨ëœ ëª¨ë¸ ë°˜í™˜


#EPOCHS = 5 
#LEARN_RATE = 0.00001
def Execute_Model(EPOCHS,LEARN_RATE):
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì´ë¯¸ì§€ ë¶ˆëŸ¬ ì˜¤ê¸°.
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    g_train_images = load_images_from_folder(train_dir)
    g_train_cleaned_images = load_images_from_folder(train_cleaned_dir)
    g_test_images = load_images_from_folder(test_dir)
    print(f"Train: {len(g_train_images)}")
    print(f"Train Cleaned: {len(g_train_cleaned_images)}")
    print(f"Test: {len(g_test_images)}")
    Lines("Create Data loader. ")
    # ë°ì´í„°ì…‹ ìƒì„±
    pairedDataset = PairedImageDataset(train_dir, train_cleaned_dir, g_transforms)
    #test_dataset = loadImageDataset(test_dir, g_transforms) 
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # PairedImageDatasetì—ì„œ ì´ë¯¸ì§€ ì‹œê°í™”
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pairedLoader = DataLoader(pairedDataset, batch_size=16, shuffle=True, num_workers=0)
    view_paired_dataset(pairedLoader, numImages=5)
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í›ˆë ¨ 
    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Lines("ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    return ExtraTrain(pairedLoader, EPOCHS, LEARN_RATE)
    

def evalModel(model,className,epoches,learnRate):    
# í…ŒìŠ¤íŠ¸
    model.eval()
    testDataset = loadImageDataset(test_dir, g_transforms) 
    testLoader = DataLoader(testDataset, batch_size=16, shuffle=False, num_workers = 0)
    with torch.no_grad():
        nIndex = 1
        for images in testLoader:
            images_batch = images.to(g_Device) # ì›ë³¸ ì´ë¯¸ì§€
            outputs_batch = model(images_batch) # ì¶œë ¥
            saveResultFile(nIndex, images_batch, outputs_batch,className,epoches,learnRate)
            visualize_images_and_outputs(images, outputs_batch)
            nIndex +=1
    Lines("View images/outputs")
def visualize_images_and_outputs(images, outputs):
    """
    ì´ë¯¸ì§€ì™€ ì¶œë ¥ ì´ë¯¸ì§€ë¥¼ ì—´ë¡œ êµ¬ë¶„í•˜ì—¬ ì‹œê°í™”.
    :param images: ì›ë³¸ ì´ë¯¸ì§€ í…ì„œ
    :param outputs: ëª¨ë¸ ì¶œë ¥ í…ì„œ
    """
    num_images = images.size(0)  # ì „ì²´ ì´ë¯¸ì§€ ê°œìˆ˜
    fig, axes = plt.subplots(num_images, 2, figsize=(10, num_images * 3))  # num_images í–‰, 2ì—´

    for i in range(num_images):
        # ì²« ë²ˆì§¸ ì—´: ì›ë³¸ ì´ë¯¸ì§€
  #     axes[i, 0].imshow(images[i].cpu().numpy().squeeze(), cmap='gray')
        axes[i, 0].imshow(images[i].cpu().numpy().squeeze().transpose((1, 2, 0)))
        axes[i, 0].set_title(f"Original {i + 1}", fontsize=10)
        axes[i, 0].axis('off')

        # ë‘ ë²ˆì§¸ ì—´: ì¶œë ¥ ì´ë¯¸ì§€
        #axes[i, 1].imshow(outputs[i].cpu().detach().numpy().squeeze(), cmap='gray')
        axes[i, 1].imshow(outputs[i].cpu().numpy().squeeze().transpose((1, 2, 0)))
        axes[i, 1].set_title(f"Output {i + 1}", fontsize=10)
        axes[i, 1].axis('off')

    plt.tight_layout()
    plt.show(block = False)
    plt.pause(3)
    plt.close()

def saveResultFile(nIndex, images, outputs, className, epoches, learnRate):
    """
    ì›ë³¸ ì´ë¯¸ì§€ì™€ ì¶œë ¥ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ í•©ì³ì„œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    # í…ì„œë¥¼ CPUë¡œ ì´ë™í•˜ê³  NumPy ë°°ì—´ë¡œ ë³€í™˜
    # detach()ëŠ” outputsì—ë§Œ ì ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. imagesëŠ” ë³´í†µ require_grad=Falseì´ë¯€ë¡œ ê´œì°®ì§€ë§Œ, ëª…í™•í•˜ê²Œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    images_np = images.cpu().numpy()
    outputs_np = outputs.cpu().detach().numpy()

    # ë°°ì¹˜ì˜ ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ë°˜ë³µ
    for i in range(images_np.shape[0]):
        # 0-1 ë²”ìœ„ë¥¼ 0-255 ë²”ìœ„ë¡œ ë³€í™˜í•˜ê³  uint8 íƒ€ì…ìœ¼ë¡œ ë³€ê²½
        
        # 1. squeeze() ì ìš© (ë¶ˆí•„ìš”í•œ ë°°ì¹˜ ì°¨ì› ì œê±°)
        img_temp = images_np[i].squeeze()
        out_temp = outputs_np[i].squeeze()
        
        # 2. ì±„ë„ ìˆœì„œ ë³€ê²½ (C, H, W -> H, W, C)
        # ë§Œì•½ ì´ë¯¸ì§€ê°€ í‘ë°±(H, W)ì´ë¼ë©´ transposeëŠ” í•„ìš” ì—†ìœ¼ë©° ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ë”°ë¼ì„œ ì°¨ì›ì´ 3ê°œì¸ ê²½ìš°(ì»¬ëŸ¬ ì´ë¯¸ì§€)ì—ë§Œ transposeë¥¼ ì ìš©í•©ë‹ˆë‹¤.
        if img_temp.ndim == 3: # ì»¬ëŸ¬ ì´ë¯¸ì§€ (C, H, W)ì¸ ê²½ìš°
            img_temp = img_temp.transpose((1, 2, 0))
            out_temp = out_temp.transpose((1, 2, 0))
            
        # 3. 0-255 ë²”ìœ„ë¡œ ë³€í™˜ ë° uint8 íƒ€ì…ìœ¼ë¡œ ë³€ê²½
        img = (img_temp * 255).astype(np.uint8)
        out = (out_temp * 255).astype(np.uint8)

        # ì›ë³¸ê³¼ ì¶œë ¥ì„ ìˆ˜í‰ìœ¼ë¡œ ì—°ê²°
        # ë‘ ë°°ì—´ì˜ (ë†’ì´, ì±„ë„)ì´ ê°™ì•„ì•¼ hstackì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        combined_img = np.hstack((img, out))
        
        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        imagedir = f"{base_dir}/model_result_image"
        save_dir = f"{imagedir}/{className}_{epoches}_{learnRate}"
        os.makedirs(save_dir, exist_ok=True) 

        # íŒŒì¼ ì´ë¦„ ì„¤ì • (ë°°ì¹˜ ì¸ë±ìŠ¤_ì´ë¯¸ì§€ ì¸ë±ìŠ¤)
        file_name = f"{save_dir}/result_{nIndex-1}_{i}.png"

        print(f"out shape: {out.shape}, out dtype: {out.dtype}, out min: {out.min()}, out max: {out.max()}")        
        # PILì„ ì‚¬ìš©í•˜ì—¬ ì €ì¥: ì´ì œ combined_imgì˜ í˜•íƒœëŠ” (Height, Width, Channels)ê°€ ë©ë‹ˆë‹¤.
        Image.fromarray(combined_img).save(file_name)
MODEL_CLASS_NAME  = "DenoisingModel"
EPOCHES = 5
LEARN_RATE = 0.00001

MODEL_PATH = f"{base_dir}/modelfiles/{MODEL_CLASS_NAME}_{EPOCHES}_{LEARN_RATE}.pth"

def GetModel():
    # ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ í›ˆë ¨
    if os.path.exists(MODEL_PATH):
        print(f"'{MODEL_PATH}' íŒŒì¼ì´ ì¡´ì¬í•˜ì—¬, ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
        # ì €ì¥ëœ ëª¨ë¸ ì „ì²´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        model = torch.load(MODEL_PATH, map_location=g_Device, weights_only=False)
        return model
    else:
        print(f"'{MODEL_PATH}' íŒŒì¼ì´ ì—†ì–´, ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        # ëª¨ë¸ í›ˆë ¨
        model = Execute_Model(EPOCHES,LEARN_RATE)
        # í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥
        torch.save(model, MODEL_PATH)
        print(f"í›ˆë ¨ëœ ëª¨ë¸ì„ '{MODEL_PATH}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        return model
model_class = GetModel()
evalModel(model_class,MODEL_CLASS_NAME,EPOCHES,LEARN_RATE)
