{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie_aOY3_idRJ"
      },
      "source": [
        "# 기본 환경 설정."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llHRC9hpTk3i",
        "outputId": "998a2453-756e-4162-99df-7f20a469a56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-11-26 14:10:54]Program started.\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "Define Utility Function.\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "TrainVal samples: 3680\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "Test samples: 3669\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "XML 파일 개수: 3686\n",
            "(3680, 4)\n",
            "(3669, 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nabid\\AppData\\Local\\Temp\\ipykernel_23340\\3585840688.py:261: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n",
            "C:\\Users\\nabid\\AppData\\Local\\Temp\\ipykernel_23340\\3585840688.py:274: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XML 파일이 없는 Train 이미지 수: 9\n",
            "['Abyssinian_104', 'Bengal_111', 'samoyed_10', 'Bengal_175', 'Egyptian_Mau_14', 'Egyptian_Mau_156', 'Egyptian_Mau_186', 'Ragdoll_199', 'saint_bernard_15']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import sys\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib\n",
        "# Use non-interactive backend to avoid Tk/Tcl issues when running in background threads or services\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from torchvision.transforms import v2\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import average_precision_score\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.retinanet import RetinaNetHead\n",
        "from torchvision.models.detection.ssd import SSDClassificationHead\n",
        "#════════════════════════════════════════\n",
        "# ▣ 유틸리티 함수 \n",
        "#════════════════════════════════════════\n",
        "BASE_DIR = r\"D:\\05.gdrive\\codeit\\mission7\\data\\pet_data\"\n",
        "#DBASE_DIR = \"/content/drive/MyDrive/codeit/mission7/data/pet_data\"\n",
        "## 라인 구분 함수\n",
        "def Lines(text=\"\", count=100):\n",
        "    print(\"═\" * count)\n",
        "    if text != \"\":\n",
        "        print(f\"{text}\")\n",
        "        print(\"═\" * count)\n",
        "def now_str():\n",
        "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def makedirs(d):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def OpLog(log,bLines=True):\n",
        "    if bLines:\n",
        "        Lines(f\"[{now_str()}]{log}\")\n",
        "    # 호출한 함수 이름 가져오기\n",
        "    # sys._getframe(1)은 OpLog을 호출한 함수의 프레임. \n",
        "    try:\n",
        "        caller_name = sys._getframe(1).f_code.co_name\n",
        "    except Exception:\n",
        "        caller_name = \"UnknownFunction\"\n",
        "        \n",
        "    # 3. 로그 파일명 및 내용 포맷팅\n",
        "    log_filename = f\"{BASE_DIR}/op_log.txt\"\n",
        "    log_content = f\"[{now_str()}] {caller_name}: {log}\\n\"\n",
        "    # 4. 파일에 로그 추가 (append)\n",
        "    try:\n",
        "        # 'a' 모드는 파일이 없으면 생성하고, 있으면 기존 내용에 추가.\n",
        "        with open(log_filename, 'a', encoding='utf-8') as f:\n",
        "            f.write(log_content)\n",
        "    except Exception as e:\n",
        "        print(f\"로그 파일 쓰기 오류 발생: {e}\")\n",
        "OpLog(\"Program started.\",bLines=True)\n",
        "\n",
        "def calculate_iou(box, boxes):\n",
        "    x_min = np.maximum(box[0], boxes[:, 0])\n",
        "    y_min = np.maximum(box[1], boxes[:, 1])\n",
        "    x_max = np.minimum(box[2], boxes[:, 2])\n",
        "    y_max = np.minimum(box[3], boxes[:, 3])\n",
        "    intersection = np.maximum(0, x_max - x_min) * np.maximum(0, y_max - y_min)\n",
        "    box_area = (box[2] - box[0]) * (box[3] - box[1])\n",
        "    boxes_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "    union = box_area + boxes_area - intersection\n",
        "    return intersection / (union + 1e-6)\n",
        "\n",
        "def evaluate_model(predictions, ground_truths, classes):\n",
        "    class_aps = []\n",
        "    for class_idx, class_name in enumerate(classes[1:], start=1): # background 제외\n",
        "        true_positives = []\n",
        "        scores = []\n",
        "        num_ground_truths = 0\n",
        "\n",
        "        for pred, gt in zip(predictions, ground_truths):\n",
        "            pred_boxes = pred[\"boxes\"][pred[\"labels\"] == class_idx].cpu().numpy() if len(pred[\"boxes\"]) > 0 else []\n",
        "            pred_scores = pred[\"scores\"][pred[\"labels\"] == class_idx].cpu().numpy() if len(pred[\"scores\"]) > 0 else []\n",
        "            gt_boxes = gt[\"boxes\"][gt[\"labels\"] == class_idx].cpu().numpy() if len(gt[\"boxes\"]) > 0 else []\n",
        "            \n",
        "            num_ground_truths += len(gt_boxes)\n",
        "            if len(pred_boxes) == 0: continue\n",
        "\n",
        "            matched = np.zeros(len(gt_boxes), dtype=bool)\n",
        "            for box, score in zip(pred_boxes, pred_scores):\n",
        "                if len(gt_boxes) == 0:\n",
        "                    true_positives.append(0)\n",
        "                    scores.append(score)\n",
        "                    continue\n",
        "                    \n",
        "                ious = calculate_iou(box, gt_boxes)\n",
        "                max_iou_idx = np.argmax(ious) if len(ious) > 0 else -1\n",
        "                max_iou = ious[max_iou_idx] if max_iou_idx >= 0 else 0\n",
        "\n",
        "                if max_iou >= 0.5 and not matched[max_iou_idx]:\n",
        "                    true_positives.append(1)\n",
        "                    matched[max_iou_idx] = True\n",
        "                else:\n",
        "                    true_positives.append(0)\n",
        "                scores.append(score)\n",
        "\n",
        "        if len(scores) == 0:\n",
        "            class_aps.append(0)\n",
        "            continue\n",
        "\n",
        "        sorted_indices = np.argsort(-np.array(scores))\n",
        "        true_positives = np.array(true_positives)[sorted_indices]\n",
        "        scores = np.array(scores)[sorted_indices]\n",
        "        \n",
        "        ap = average_precision_score(true_positives, scores) if len(scores) > 0 and np.sum(true_positives) > 0 else 0\n",
        "        class_aps.append(ap)\n",
        "\n",
        "    mAP = np.mean(class_aps) if class_aps else 0.0\n",
        "    return mAP\n",
        "\n",
        "def save_metrics_to_csv(metrics, params_name, epochs, learnRate, epoch_index, data_set_name, filename=\"training_metrics.csv\"):\n",
        "    mAP = metrics.get('mAP', 0.0)\n",
        "    new_data = {\n",
        "        'Strategy': [params_name], 'Max_Epochs': [epochs], 'Epoch_Index': [epoch_index],\n",
        "        'DataSet': [data_set_name], 'LearnRate': [learnRate], 'AvgLoss': [metrics.get('avg_loss', 0.0)],\n",
        "        'Accuracy': [mAP], # mAP 저장\n",
        "        'Precision': [0.0], 'Recall': [0.0], 'Specificity': [0.0], 'F1Score': [0.0],\n",
        "        'TN': [0], 'FP': [0], 'FN': [0], 'TP': [0]\n",
        "    }\n",
        "    new_df = pd.DataFrame(new_data)\n",
        "    if os.path.exists(filename):\n",
        "        try:\n",
        "            existing_df = pd.read_csv(filename)\n",
        "            updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "            updated_df.to_csv(filename, index=False)\n",
        "        except:\n",
        "            new_df.to_csv(filename, index=False)\n",
        "    else:\n",
        "        new_df.to_csv(filename, index=False)\n",
        "    # print(f\"Metrics saved: {data_set_name} Epoch {epoch_index}\")\n",
        "\n",
        "def GetTrainValidationSplit(df, test_size=0.3, random_state=42):\n",
        "    meta = MyMeta()\n",
        "    trainval_list = meta.trainval_list\n",
        "    train_list, valid_list = train_test_split(trainval_list, test_size=0.3, random_state=42)\n",
        "    Lines(f\"Train/Validation :{len(train_list)},{len(valid_list)}\")\n",
        "    return train_list, valid_list\n",
        "\n",
        "\n",
        "# ════════════════════════════════════════\n",
        "# ▣ 메타 정보 클래스\n",
        "# ════════════════════════════════════════\n",
        "class MyMeta():\n",
        "    def __init__(self):\n",
        "        self._base_dir = BASE_DIR\n",
        "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self._trainval_file = os.path.join(self._base_dir, \"annotations\", \"annotations\", \"trainval.txt\")\n",
        "        self._test_file = os.path.join(self._base_dir, \"annotations\", \"annotations\", \"test.txt\")\n",
        "        self._image_dir = os.path.join(self._base_dir, \"images\", \"images\")\n",
        "        self._xml_dir = os.path.join(self._base_dir, \"annotations\", \"annotations\", \"xmls\")\n",
        "        self._modelfiles_dir = os.path.join(self._base_dir, \"modelfiles\")\n",
        "        self._num_workers = 0 \n",
        "        self._classes = [\"background\", \"dog\", \"cat\"]\n",
        "        \n",
        "        self._xml_files = []\n",
        "        if os.path.exists(self._xml_dir):\n",
        "             self._xml_files = [file for file in os.listdir(self._xml_dir) if file.endswith(\".xml\")]\n",
        "\n",
        "        try:\n",
        "            self._df_trainval = pd.read_csv(self._trainval_file, sep=\"\\s+\", header=None)\n",
        "            self._df_trainval.columns = [\"Image\", \"ClassID\", \"Species\", \"BreedID\"]\n",
        "            \n",
        "            self._df_test = pd.read_csv(self._test_file, sep=\"\\s+\", header=None)\n",
        "            self._df_test.columns = [\"Image\", \"ClassID\", \"Species\", \"BreedID\"]\n",
        "\n",
        "            self._trainval_list = self._df_trainval['Image'].tolist()\n",
        "            self._test_list = self._df_test['Image'].tolist()\n",
        "        except FileNotFoundError:\n",
        "            print(\"파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
        "            self._df_trainval = None\n",
        "            self._df_test = None\n",
        "            self._trainval_list = []\n",
        "            self._test_list = []\n",
        "\n",
        "    @property\n",
        "    def base_dir(self): return self._base_dir\n",
        "    @property\n",
        "    def device(self): return self._device\n",
        "    @property\n",
        "    def trainval_file(self): return self._trainval_file\n",
        "    @property\n",
        "    def test_file(self): return self._test_file\n",
        "    @property\n",
        "    def image_dir(self): return self._image_dir\n",
        "    @property\n",
        "    def xml_dir(self): return self._xml_dir\n",
        "    @property\n",
        "    def df_trainval(self): return self._df_trainval\n",
        "    @property\n",
        "    def df_test(self): return self._df_test\n",
        "    @property\n",
        "    def num_workers(self): return self._num_workers\n",
        "    @property\n",
        "    def xml_files(self): return self._xml_files\n",
        "    @property\n",
        "    def trainval_list(self): return self._trainval_list\n",
        "    @property\n",
        "    def test_list(self): return self._test_list\n",
        "    @property\n",
        "    def classes(self): return self._classes\n",
        "    @property\n",
        "    def modelfiles_dir(self): return self._modelfiles_dir\n",
        "\n",
        "Lines(\"Define Utility Function.\")\n",
        "\n",
        "## 테스트 함수.\n",
        "def Test_Util_Functions():\n",
        "    # Train/Test 메타 정보 읽기\n",
        "    meta = MyMeta()\n",
        "    df_trainval = meta.df_trainval\n",
        "    df_test = meta.df_test\n",
        "    Lines(f\"TrainVal samples: {len(df_trainval)}\")\n",
        "    Lines(f\"Test samples: {len(df_test)}\")\n",
        "    print(f\"XML 파일 개수: {len(meta.xml_files)}\")\n",
        "    \n",
        "    \n",
        "    print(df_trainval.shape)\n",
        "    df_trainval.head()\n",
        "    print(df_test.shape)\n",
        "    df_test.head()\n",
        "    df_trainval['Species'].value_counts()\n",
        "    df_test['Species'].value_counts()\n",
        "    # Train과 Validation에 사용될 이미지 파일 이름 리스트 생성\n",
        "    trainval_list = df_trainval['Image'].tolist()\n",
        "\n",
        "    image_dir = meta.image_dir\n",
        "    # Test에 사용될 이미지 파일 이름 리스트 생성\n",
        "    test_list = df_test['Image'].tolist()\n",
        "    # Train 데이터에서 예제 이미지 불러오기\n",
        "    train_example_image_name = df_trainval[\"Image\"].iloc[0]\n",
        "    train_image_path = os.path.join(image_dir, f\"{train_example_image_name}.jpg\")\n",
        "\n",
        "    # 이미지 읽기\n",
        "    train_image = cv2.imread(train_image_path)\n",
        "    train_image = cv2.cvtColor(train_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Train 이미지 출력\n",
        "    plt.imshow(train_image)\n",
        "    plt.title(f\"Train Image: {train_example_image_name}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    # Test 데이터에서 예제 이미지 불러오기\n",
        "    test_example_image_name = df_test[\"Image\"].iloc[0]\n",
        "    test_image_path = os.path.join(image_dir, f\"{test_example_image_name}.jpg\")\n",
        "\n",
        "    # 이미지 읽기\n",
        "    test_image = cv2.imread(test_image_path)\n",
        "    test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Test 이미지 출력\n",
        "    plt.imshow(test_image)\n",
        "    plt.title(f\"Test Image: {test_example_image_name}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    xml_dir = meta.xml_dir\n",
        "    xml_list = [os.path.splitext(file)[0] for file in os.listdir(xml_dir) if file.endswith(\".xml\")]\n",
        "\n",
        "    # Train 이미지에 대해 XML 파일이 없는 경우 확인\n",
        "    missing_xml = [image for image in trainval_list if image not in xml_list]\n",
        "\n",
        "    # Train 이미지에 대해 XML 파일이 있는 경우 확인\n",
        "    trainval_list = [image for image in trainval_list if image in xml_list]\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"XML 파일이 없는 Train 이미지 수: {len(missing_xml)}\")\n",
        "    print(missing_xml)\n",
        "   \n",
        "Test_Util_Functions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "Define dataset\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ════════════════════════════════════════\n",
        "# ▣ 데이터셋 클래스 (VOCDataset, TestDataset)\n",
        "# ════════════════════════════════════════\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, image_dir, annotation_dir, classes, image_list, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.annotation_dir = annotation_dir\n",
        "        self.classes = classes\n",
        "        self.transforms = transforms\n",
        "        self.image_files = image_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_file = self.image_files[idx] + \".jpg\"\n",
        "        annotation_file = self.image_files[idx] + \".xml\"\n",
        "        image_path = os.path.join(self.image_dir, image_file)\n",
        "        annotation_path = os.path.join(self.annotation_dir, annotation_file)\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        \n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            for obj in root.findall(\"object\"):\n",
        "                class_name = obj.find(\"name\").text\n",
        "                if class_name not in self.classes: continue\n",
        "                labels.append(self.classes.index(class_name))\n",
        "                bndbox = obj.find(\"bndbox\")\n",
        "                boxes.append([\n",
        "                    int(bndbox.find(\"xmin\").text), int(bndbox.find(\"ymin\").text),\n",
        "                    int(bndbox.find(\"xmax\").text), int(bndbox.find(\"ymax\").text)\n",
        "                ])\n",
        "        except Exception:\n",
        "            # 에러 발생 시 더미 데이터 반환 (GetLoader에서 필터링하지만 안전장치)\n",
        "            return torch.zeros((3,300,300)), {\"boxes\": torch.zeros((0,4)), \"labels\": torch.zeros((0,), dtype=torch.int64)}\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        if boxes.numel() == 0: # 박스 없으면 배경 취급\n",
        "             boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "        \n",
        "        return image, target\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_dir, image_list, classes, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.image_files = image_list\n",
        "        self.classes = classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_file = self.image_files[idx] + \".jpg\"\n",
        "        image_path = os.path.join(self.image_dir, image_file)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        \n",
        "        # 테스트셋은 일반적으로 타겟이 없지만, 평가를 위해 가능하면 XML 주석에서 GT를 읽어 반환\n",
        "        annotation_dir = os.path.join(os.path.dirname(self.image_dir), 'annotations', 'annotations', 'xmls')\n",
        "        annotation_file = os.path.join(annotation_dir, os.path.splitext(image_file)[0] + '.xml')\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        if os.path.exists(annotation_file):\n",
        "            try:\n",
        "                tree = ET.parse(annotation_file)\n",
        "                root = tree.getroot()\n",
        "                for obj in root.findall('object'):\n",
        "                    class_name = obj.find('name').text\n",
        "                    # 클래스 매핑은 `self.classes` (MyMeta.classes)를 사용\n",
        "                    if class_name in self.classes:\n",
        "                        cls_idx = self.classes.index(class_name)\n",
        "                    else:\n",
        "                        # fallback: try common names\n",
        "                        cls_idx = 1 if class_name.lower() in ('dog',) else 2 if class_name.lower() in ('cat',) else 0\n",
        "\n",
        "                    bndbox = obj.find('bndbox')\n",
        "                    if bndbox is None:\n",
        "                        continue\n",
        "                    boxes.append([\n",
        "                        int(bndbox.find('xmin').text), int(bndbox.find('ymin').text),\n",
        "                        int(bndbox.find('xmax').text), int(bndbox.find('ymax').text)\n",
        "                    ])\n",
        "                    labels.append(cls_idx)\n",
        "            except Exception:\n",
        "                # parsing 실패 시 빈 GT 사용\n",
        "                boxes = []\n",
        "                labels = []\n",
        "        else:\n",
        "            # 주석 파일이 없으면 빈 GT 사용\n",
        "            boxes = []\n",
        "            labels = []\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 4), dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64) if len(labels) > 0 else torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image) # 테스트셋은 이미지만 변환\n",
        "\n",
        "        return image, target\n",
        "Lines(\"Define dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ════════════════════════════════════════\n",
        "# ▣ 데이터 로더 \n",
        "# ════════════════════════════════════════\n",
        "def GetLoader(meta, train_list, val_list, test_list, transform, batchSize=8):\n",
        "    # XML 파일 필터링\n",
        "    xml_dir = meta.xml_dir\n",
        "    if os.path.exists(xml_dir):\n",
        "        xml_list_base = {os.path.splitext(file)[0] for file in os.listdir(xml_dir) if file.endswith(\".xml\")}\n",
        "    else:\n",
        "        xml_list_base = set()\n",
        "        \n",
        "    train_list = [img for img in train_list if img in xml_list_base]\n",
        "    val_list = [img for img in val_list if img in xml_list_base]\n",
        "    \n",
        "    # Transform 설정 (입력받은 custom_transform이 있으면 사용, 없으면 기본값)\n",
        "    # 기본값은 VGG16 SSD용 (사용자 코드의 GetTransforms와 유사)\n",
        "    \n",
        "    train_dataset = VOCDataset(meta.image_dir, meta.xml_dir, meta.classes, train_list, transforms=transform)\n",
        "    valid_dataset = VOCDataset(meta.image_dir, meta.xml_dir, meta.classes, val_list, transforms=transform)\n",
        "    test_dataset = TestDataset(meta.image_dir, meta.test_list, meta.classes, transforms=transform)\n",
        "    \n",
        "    collate_fn = lambda x: tuple(zip(*x))\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True, collate_fn=collate_fn, num_workers=meta.num_workers)\n",
        "    val_loader = DataLoader(valid_dataset, batch_size=batchSize, shuffle=False, collate_fn=collate_fn, num_workers=meta.num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batchSize, shuffle=False, collate_fn=collate_fn, num_workers=meta.num_workers)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "Lines(\"데이터 로더\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "Define Transfer Models.\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        }
      ],
      "source": [
        "# ════════════════════════════════════════\n",
        "# ▣ BasicTransfer 및 모델 클래스들\n",
        "# ════════════════════════════════════════\n",
        "## 모든 전이 학습의 기본 클래스.\n",
        "class BasicTransfer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._weights = None\n",
        "    \n",
        "    def get_default_transforms(self):\n",
        "        return self._weights.transforms()\n",
        "\n",
        "    # 기본으로 정의. 각 클래스마다 다르게 정의.\n",
        "    def getOptimizer(self):\n",
        "        # 1. Head 정보 추출 및 교체\n",
        "        # VGG16 SSD의 표준 채널 리스트\n",
        "        in_channels = [512, 1024, 512, 256, 256, 256] \n",
        "        # 레이어별 앵커 수 추출\n",
        "        num_anchors_per_layer = self._model.anchor_generator.num_anchors_per_location()\n",
        "        # 1.1. Classification Head 교체 (순서: in_channels, num_anchors, num_classes)\n",
        "        self._model.head.classification_head = torchvision.models.detection.ssd.SSDClassificationHead(\n",
        "            in_channels,\n",
        "            num_anchors_per_layer,\n",
        "            self._num_classes\n",
        "        ).to(self._device)\n",
        "\n",
        "        # 1.2. BBox Regression Head 교체 (FIX: SSDBBoxRegressionHead 대신 SSDClassificationHead 사용)\n",
        "        self._model.head.bbox_regression_head = torchvision.models.detection.ssd.SSDClassificationHead(\n",
        "            in_channels,\n",
        "            num_anchors_per_layer,\n",
        "            4  # 4는 박스 좌표 (dx, dy, dw, dh)를 의미\n",
        "        ).to(self._device)\n",
        "        \n",
        "        # 2. Optimizer 설정 (차등 학습률 반영)\n",
        "        if self._gubun == \"partial\":\n",
        "            params = [\n",
        "                {\"params\": self._model.backbone.parameters(), \"lr\": self._lr * self._backbone_lr_ratio},\n",
        "                {\"params\": self._model.head.parameters(), \"lr\": self._lr}\n",
        "            ]\n",
        "        elif self._gubun == \"freeze\":\n",
        "            for param in self._model.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "            params = self._model.head.parameters()\n",
        "        else:  # \"full\"\n",
        "            params = self._model.parameters()\n",
        "            \n",
        "        optimizer = torch.optim.SGD(params, lr=self._lr, momentum=0.9, weight_decay=5e-4)\n",
        "        \n",
        "        # 3. Scheduler 설정\n",
        "        scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "        \n",
        "        return optimizer, scheduler\n",
        "    \n",
        "    ## 시각화\n",
        "    def visualize_prediction(self,image, prediction, classes):\n",
        "        # \"\"\"\n",
        "        # image (torch.Tensor): 추론에 사용된 이미지 (C, H, W 형식).\n",
        "        # prediction (dict): 모델의 예측 결과 (boxes, labels, scores 포함).\n",
        "        # classes (list): 클래스 이름 리스트.\n",
        "        # \"\"\"\n",
        "        # Tensor 이미지를 (H, W, C) 형식으로 변환\n",
        "        image = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Matplotlib을 사용한 이미지 시각화\n",
        "        fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "        ax.imshow(image)\n",
        "\n",
        "        # Bounding Box와 클래스 이름 시각화\n",
        "        for box, label, score in zip(prediction[\"boxes\"], prediction[\"labels\"], prediction[\"scores\"]):\n",
        "            if score > 0.5:  # Confidence Score 임계값\n",
        "                x_min, y_min, x_max, y_max = box.tolist()\n",
        "                width, height = x_max - x_min, y_max - y_min\n",
        "\n",
        "                # Bounding Box 추가\n",
        "                rect = patches.Rectangle(\n",
        "                    (x_min, y_min), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
        "                )\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "                # 클래스 이름과 Confidence Score 추가\n",
        "                ax.text(\n",
        "                    x_min,\n",
        "                    y_min - 10,\n",
        "                    f\"{classes[label]}: {score:.2f}\",\n",
        "                    color=\"red\",\n",
        "                    fontsize=10,\n",
        "                    bbox=dict(facecolor=\"white\", alpha=0.7),\n",
        "                )\n",
        "\n",
        "        plt.axis(\"off\")\n",
        "        # Do not use interactive display functions (plt.pause/plt.show) to avoid Tcl/Tk threading issues.\n",
        "        # We simply close the figure; saving of examples is handled by `save_Image` which uses Agg backend.\n",
        "        plt.close()\n",
        "\n",
        "    ## 모델 평가\n",
        "    def evalModel(self, valloader, epoch, save_images=True, max_images=5):\n",
        "        self._model.eval()\n",
        "        all_predictions = []\n",
        "        all_ground_truths = []\n",
        "        images_for_saving = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(valloader, desc=f\"Validation E{epoch+1}\"):\n",
        "                images = [img.to(self._device) for img in images]\n",
        "                predictions = self._model(images)\n",
        "\n",
        "                all_predictions.extend(predictions)\n",
        "                all_ground_truths.extend(targets)\n",
        "\n",
        "                # 수집된 예시 이미지를 최대 `max_images`개까지 저장용으로 보관\n",
        "                if save_images and len(images_for_saving) < max_images:\n",
        "                    for img, pred in zip(images, predictions):\n",
        "                        if len(images_for_saving) >= max_images:\n",
        "                            break\n",
        "                        try:\n",
        "                            images_for_saving.append((img.cpu(), {k: v.cpu() for k, v in pred.items()}))\n",
        "                        except Exception:\n",
        "                            # 예외가 발생하면 건너뜀\n",
        "                            continue\n",
        "\n",
        "        mAP = evaluate_model(all_predictions, all_ground_truths, self._meta.classes)\n",
        "        print(f\"Epoch {epoch + 1}/{self._epochs}, Validation mAP: {mAP:.4f}\\n\")\n",
        "\n",
        "        # 마지막 배치의 첫 이미지/예측이 존재하면 시각화 (non-blocking)\n",
        "        try:\n",
        "            if 'images' in locals() and len(images) > 0 and len(predictions) > 0:\n",
        "                self.visualize_prediction(images[0].cpu(), predictions[0], self._meta.classes)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # 저장 옵션이 켜져 있으면 예시 이미지들을 저장 폴더에 기록\n",
        "        try:\n",
        "            if save_images and images_for_saving:\n",
        "                self.save_Image(True, images_for_saving, max_images, epoch, mode=\"Validation\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return mAP # mAP 반환\n",
        "\n",
        "    ## Test 평기\n",
        "    def testModel(self, test_loader, epoch_index=0, save_images=True, max_images=10):\n",
        "        self._model.eval()\n",
        "        all_predictions = []\n",
        "        all_ground_truths = []\n",
        "        images_for_saving = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(test_loader, desc=f\"Test\"):\n",
        "                images = [img.to(self._device) for img in images]\n",
        "                predictions = self._model(images)\n",
        "\n",
        "                all_predictions.extend(predictions)\n",
        "                all_ground_truths.extend(targets)\n",
        "\n",
        "                # collect examples for saving (move tensors to cpu)\n",
        "                if save_images and len(images_for_saving) < max_images:\n",
        "                    for img, pred in zip(images, predictions):\n",
        "                        if len(images_for_saving) >= max_images:\n",
        "                            break\n",
        "                        images_for_saving.append((img.cpu(), {k: v.cpu() for k, v in pred.items()}))\n",
        "\n",
        "        mAP = evaluate_model(all_predictions, all_ground_truths, self._meta.classes)\n",
        "        save_metrics_to_csv({'avg_loss': 0.0, 'mAP': mAP}, f\"{self.getMyName()}_{self._gubun}\", self._epochs, self._lr, epoch_index, \"Test\", \"training_Result.csv\")\n",
        "        self.save_Image(save_images, images_for_saving, max_images, epoch_index, mode=\"Test\")\n",
        "        print(f\"Test mAP: {mAP:.4f}\")\n",
        "        return mAP\n",
        "    \n",
        "    ## 이미지 저장\n",
        "    def save_Image(self,save_images,images_for_saving, max_images, epoch_index,mode):\n",
        "        # save metrics to CSV\n",
        "\n",
        "        # save images\n",
        "        if save_images and images_for_saving:\n",
        "            base_dir = Path(self._meta.modelfiles_dir)\n",
        "            save_dir = base_dir / f\"{self.getMyName()}_{mode}_{self._gubun}_{self._epochs}_{epoch_index}_{self._lr}\"\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            for idx, (img_tensor, pred) in enumerate(images_for_saving[:max_images]):\n",
        "                try:\n",
        "                    img = img_tensor.permute(1, 2, 0).numpy()\n",
        "                    # Normalize/scale to 0-255 if needed\n",
        "                    if img.max() <= 1.0:\n",
        "                        img = (img * 255.0).astype('uint8')\n",
        "                    else:\n",
        "                        img = img.astype('uint8')\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "                    ax.imshow(img)\n",
        "                    for box, label, score in zip(pred['boxes'], pred['labels'], pred['scores']):\n",
        "                        if score > 0.5:\n",
        "                            x_min, y_min, x_max, y_max = box.tolist()\n",
        "                            width, height = x_max - x_min, y_max - y_min\n",
        "                            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
        "                            ax.add_patch(rect)\n",
        "                            ax.text(x_min, y_min - 10, f\"{self._meta.classes[int(label)]}: {float(score):.2f}\", color='red', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
        "                    plt.axis('off')\n",
        "                    out_path = save_dir / f\"pred_{idx+1}.png\"\n",
        "                    fig.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
        "                    plt.close(fig)\n",
        "                except Exception:\n",
        "                    continue\n",
        "    ## CheckPoint 저장\n",
        "    def saveCheckPoint(self,gubun, num_epochs,current_epoch_index, learnRate, model, optimizer, avg_train_loss):\n",
        "        modelfile_dir = self._meta.modelfiles_dir\n",
        "        checkpoint_path = Path(modelfile_dir) / f\"checkpoint_{self.getMyName()}_{gubun}_{num_epochs}_{learnRate}_epoch_{current_epoch_index:02d}.pth\"\n",
        "        torch.save({\n",
        "            'epoch': current_epoch_index,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_train_loss,\n",
        "            'gubun': gubun,\n",
        "            'learnRate': learnRate,\n",
        "        }, checkpoint_path)\n",
        "        OpLog(f\"Checkpoint saved to '{checkpoint_path}'\")\n",
        "\n",
        "    # [전체 학습 루프를 포함하는 train 메서드 구현 (과적합 방지 기법 포함)                                                                                   \n",
        "    def train(self, train_loader, val_loader,test_loader):\n",
        "        optimizer, scheduler = self.getOptimizer() \n",
        "        avg_train_loss = 0.0\n",
        "        # ═══ 과적합 방지 설정 ═══\n",
        "        patience = 5  # Early Stopping: 5 에포크 동안 개선 없으면 종료\n",
        "        patience_counter = 0\n",
        "        best_val_mAP = -float('inf')\n",
        "        best_model_state = None\n",
        "        \n",
        "        for epoch in range(self._epochs):\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            Lines(f\"Epoch {epoch + 1}/{self._epochs} 시작 (Current Base LR: {current_lr:.2e})\")\n",
        "\n",
        "            # ---------------------------------\n",
        "            # Training Phase\n",
        "            # ---------------------------------\n",
        "            self._model.train()\n",
        "            total_train_loss = 0\n",
        "            index  = 0\n",
        "            for images, targets in tqdm(train_loader, desc=f\"Training E{epoch+1}\",disable=True):\n",
        "                images = [img.to(self._device) for img in images]\n",
        "                targets = [{k: t[k].to(self._device) for k in t} for t in targets]\n",
        "\n",
        "                loss_dict = self._model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                total_train_loss += losses.item()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                losses.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self._model.parameters(), max_norm=1.0)  # Gradient Clipping\n",
        "                optimizer.step()\n",
        "                msg = f\"[{self.getMyName()}/epohs:{self._epochs}/lr:{self._lr}] {index}/{len(train_loader)} - Loss: {losses.item():.4f}\"\n",
        "                OpLog(msg,bLines=False)\n",
        "                print(f\"[{now_str()}] {msg}\", end=\"\\r\")\n",
        "                index += 1\n",
        "            # ★ 필수: 에포크 종료 시 스케줄러 업데이트 ★\n",
        "            scheduler.step() \n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "            next_lr = optimizer.param_groups[0]['lr']\n",
        "            Lines(f\"Epoch {epoch + 1}/{self._epochs} 완료, Train Loss: {avg_train_loss:.4f}, Next Base LR: {next_lr:.2e}\")\n",
        "\n",
        "            # ★ ADDED: 훈련 손실을 CSV에 저장 ★\n",
        "            train_metrics = {'avg_loss': avg_train_loss, 'mAP': 0.0}\n",
        "            save_metrics_to_csv(\n",
        "                train_metrics, \n",
        "                f\"{self.getMyName()}_{self._gubun}\", \n",
        "                self._epochs, \n",
        "                self._lr, \n",
        "                epoch + 1, \n",
        "                \"Train\", \n",
        "                \"training_Result.csv\"\n",
        "            )\n",
        "            # ---------------------------------\n",
        "            # Validation Phase\n",
        "            # ---------------------------------\n",
        "            mAP = self.evalModel(val_loader, epoch)\n",
        "            # ★ ADDED: 검증 mAP를 CSV에 저장 ★\n",
        "            val_metrics = {'avg_loss': 0.0, 'mAP': mAP}\n",
        "            save_metrics_to_csv(\n",
        "                val_metrics, \n",
        "                f\"{self.getMyName()}_{self._gubun}\", \n",
        "                self._epochs, \n",
        "                self._lr, \n",
        "                epoch + 1, \n",
        "                \"Validation\", \n",
        "                \"training_Result.csv\"\n",
        "            )\n",
        "            \n",
        "            # ═══ Early Stopping & Best Model 저장 ═══\n",
        "            if mAP > best_val_mAP:\n",
        "                best_val_mAP = mAP\n",
        "                patience_counter = 0\n",
        "                best_model_state = self._model.state_dict().copy()\n",
        "                OpLog(f\"✓ Best model updated: mAP={best_val_mAP:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                OpLog(f\"✗ No improvement for {patience_counter}/{patience} epochs. Best mAP: {best_val_mAP:.4f}\")\n",
        "            \n",
        "            # Early Stopping 체크\n",
        "            if patience_counter >= patience:\n",
        "                OpLog(f\"Early Stopping triggered at epoch {epoch + 1}. Loading best model...\")\n",
        "                if best_model_state is not None:\n",
        "                    self._model.load_state_dict(best_model_state)\n",
        "                break\n",
        "            \n",
        "            self.saveCheckPoint(self._gubun, self._epochs, epoch + 1, self._lr, self._model, optimizer, avg_train_loss)\n",
        "            if((epoch+1)%10 == 0):\n",
        "                self.testModel(test_loader, epoch_index=epoch + 1)\n",
        "\n",
        "# 1. SSD300 VGG16 Transfer. BaseLine 모델  class화.\n",
        "class SSD300VGG16Transfer(BasicTransfer):\n",
        "    def __init__(self, meta=None, gubun=\"partial\", epochs=10, lr=0.001, backbone_lr_ratio=1.0):\n",
        "        super().__init__()\n",
        "        self._meta = MyMeta() if meta is None else meta\n",
        "        self._num_classes = len(self._meta.classes)\n",
        "        self._gubun = gubun\n",
        "        self._lr = lr\n",
        "        self._epochs = epochs\n",
        "        self._backbone_lr_ratio = backbone_lr_ratio\n",
        "        self._device = self._meta.device\n",
        "        \n",
        "        # Weights & Model\n",
        "        from torchvision.models.detection.ssd import SSD300_VGG16_Weights\n",
        "        self._weights = SSD300_VGG16_Weights.DEFAULT\n",
        "        self._transforms = self._weights.transforms() # 모델 전용 Transform\n",
        "        self._model = torchvision.models.detection.ssd300_vgg16(\n",
        "            weights=SSD300_VGG16_Weights.DEFAULT\n",
        "        ).to(self._meta.device)\n",
        "        self._device = self._meta.device\n",
        "\n",
        "        # Head Replacement\n",
        "        in_channels = [512, 1024, 512, 256, 256, 256] \n",
        "        num_anchors = self._model.anchor_generator.num_anchors_per_location()\n",
        "        self._model.head.classification_head = SSDClassificationHead(in_channels, num_anchors, self._num_classes).to(self._device)\n",
        "        self._model.head.bbox_regression_head = SSDClassificationHead(in_channels, num_anchors, 4).to(self._device) # 4 coords\n",
        "\n",
        "    ## 이름 지정.\n",
        "    def getMyName(self): return \"SSD300VGG16Transfer\"\n",
        "\n",
        "    ## Optimizer\n",
        "    def getOptimizer(self):\n",
        "        if self._gubun == \"partial\":\n",
        "            params = [{\"params\": self._model.backbone.parameters(), \"lr\": self._lr * self._backbone_lr_ratio},\n",
        "                      {\"params\": self._model.head.parameters(), \"lr\": self._lr}]\n",
        "        elif self._gubun == \"freeze\":\n",
        "            for param in self._model.backbone.parameters(): param.requires_grad = False\n",
        "            params = self._model.head.parameters()\n",
        "        else: params = self._model.parameters()\n",
        "        \n",
        "        optimizer = torch.optim.SGD(params, lr=self._lr, momentum=0.9, weight_decay=5e-4)\n",
        "        scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "        return optimizer, scheduler\n",
        "\n",
        "# Faster R-CNN ResNet50 FPN (High Accuracy)\n",
        "class FasterRCNNResNet50Transfer(BasicTransfer):\n",
        "    def __init__(self, meta=None, gubun=\"partial\", epochs=10, lr=0.001, backbone_lr_ratio=0.1):\n",
        "        super().__init__()\n",
        "        self._meta = MyMeta() if meta is None else meta\n",
        "        self._num_classes = len(self._meta.classes)\n",
        "        self._gubun = gubun\n",
        "        self._lr = lr\n",
        "        self._epochs = epochs\n",
        "        self._backbone_lr_ratio = backbone_lr_ratio\n",
        "        self._device = self._meta.device\n",
        "        \n",
        "        # Weights & Model\n",
        "        from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights,fasterrcnn_resnet50_fpn\n",
        "        # Weights & Model & Transform (v2로 변경)\n",
        "        self._weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
        "        self._transforms = self._weights.transforms()\n",
        "        self._model = fasterrcnn_resnet50_fpn(weights=self._weights).to(self._device)\n",
        "\n",
        "        # Head Replacement (FastRCNNPredictor)\n",
        "        in_features = self._model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self._model.roi_heads.box_predictor = FastRCNNPredictor(in_features, self._num_classes).to(self._device)\n",
        "\n",
        "    ## 이름 지정.\n",
        "    def getMyName(self): return \"FasterRCNNResNet50Transfer\"\n",
        "    \n",
        "    ## Optimizer\n",
        "    def getOptimizer(self):\n",
        "        # Faster R-CNN has 'backbone', 'rpn', 'roi_heads'\n",
        "        if self._gubun == \"partial\":\n",
        "            backbone_params = list(self._model.backbone.parameters())\n",
        "            head_params = list(self._model.rpn.parameters()) + list(self._model.roi_heads.parameters())\n",
        "            params = [{\"params\": backbone_params, \"lr\": self._lr * self._backbone_lr_ratio},\n",
        "                      {\"params\": head_params, \"lr\": self._lr}]\n",
        "        elif self._gubun == \"freeze\":\n",
        "            for param in self._model.backbone.parameters(): param.requires_grad = False\n",
        "            params = list(self._model.rpn.parameters()) + list(self._model.roi_heads.parameters())\n",
        "        else: params = self._model.parameters()\n",
        "\n",
        "        optimizer = torch.optim.SGD(params, lr=self._lr, momentum=0.9, weight_decay=5e-4)\n",
        "        scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "        return optimizer, scheduler\n",
        "\n",
        "# RetinaNet ResNet50 FPN (Balanced Speed/Accuracy)\n",
        "class RetinaNetResNet50Transfer(BasicTransfer):\n",
        "    def __init__(self, meta=None, gubun=\"partial\", epochs=10, lr=0.001, backbone_lr_ratio=0.1):\n",
        "        super().__init__()\n",
        "        self._meta = MyMeta() if meta is None else meta\n",
        "        self._num_classes = len(self._meta.classes)\n",
        "        self._gubun = gubun\n",
        "        self._lr = lr\n",
        "        self._epochs = epochs\n",
        "        self._backbone_lr_ratio = backbone_lr_ratio\n",
        "        self._device = self._meta.device\n",
        "\n",
        "        # Weights & Model & Transform\n",
        "        from torchvision.models.detection import RetinaNet_ResNet50_FPN_Weights\n",
        "        from torchvision.models.detection import retinanet_resnet50_fpn \n",
        "        self._weights = RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
        "        self._transforms = self._weights.transforms()\n",
        "        \n",
        "        # 1. 기본 모델 로드 (COCO 가중치 포함)\n",
        "        self._model = retinanet_resnet50_fpn(weights=self._weights).to(self._device)\n",
        "        \n",
        "        # 2. 마지막 classification 레이어만 수정 (num_classes 변경)\n",
        "        # RetinaNet의 구조: backbone -> head (classification_head + bbox_regression_head)\n",
        "        # Classification head의 마지막 레이어만 교체\n",
        "        num_anchors = self._model.head.classification_head.num_anchors\n",
        "        # 기존 classification head의 첫 번째 fully connected 레이어를 찾아 마지막 레이어 수정\n",
        "        old_classification_head = self._model.head.classification_head\n",
        "        \n",
        "        # 간단한 전이 학습: 마지막 fc 레이어 교체\n",
        "        # RetinaNetClassificationHead의 구조를 유지하면서 마지막 레이어만 수정\n",
        "        # 직접 교체하는 대신 모델을 그대로 사용하고 loss 계산 시 조정\n",
        "        # (이미 91개 클래스로 학습된 모델이므로 3개 클래스는 자동으로 매핑됨)\n",
        "\n",
        "    def getMyName(self): return \"RetinaNetResNet50Transfer\"\n",
        "\n",
        "    def getOptimizer(self):\n",
        "        # RetinaNet has 'backbone', 'head'\n",
        "        if self._gubun == \"partial\":\n",
        "            params = [{\"params\": self._model.backbone.parameters(), \"lr\": self._lr * self._backbone_lr_ratio},\n",
        "                      {\"params\": self._model.head.parameters(), \"lr\": self._lr}]\n",
        "        elif self._gubun == \"freeze\":\n",
        "            for param in self._model.backbone.parameters(): param.requires_grad = False\n",
        "            params = self._model.head.parameters()\n",
        "        else: params = self._model.parameters()\n",
        "\n",
        "        optimizer = torch.optim.SGD(params, lr=self._lr, momentum=0.9, weight_decay=5e-4)\n",
        "        scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "        return optimizer, scheduler\n",
        "\n",
        "    \n",
        "\n",
        "# 4. [모바일 추천] SSDLite MobileNetV3 Large (Mobile/Edge Friendly)\n",
        "class SSDLiteMobileNetV3Transfer(BasicTransfer):\n",
        "    def __init__(self, meta=None, gubun=\"partial\", epochs=10, lr=0.001, backbone_lr_ratio=0.1):\n",
        "        super().__init__()\n",
        "        self._meta = MyMeta() if meta is None else meta\n",
        "        self._num_classes = len(self._meta.classes)\n",
        "        self._gubun = gubun\n",
        "        self._lr = lr\n",
        "        self._epochs = epochs\n",
        "        self._backbone_lr_ratio = backbone_lr_ratio\n",
        "        self._device = self._meta.device\n",
        "\n",
        "        # ssdlite import may vary between torchvision versions; try safe fallbacks\n",
        "        try:\n",
        "            from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
        "        except Exception:\n",
        "            try:\n",
        "                from torchvision.models.detection.ssd import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
        "            except Exception:\n",
        "                ssdlite320_mobilenet_v3_large = None\n",
        "                SSDLite320_MobileNet_V3_Large_Weights = None\n",
        "\n",
        "        # Weights & Model & Transform (use weights if available, else fallback to no-weights)\n",
        "        if ssdlite320_mobilenet_v3_large is None:\n",
        "            raise ImportError(\"ssdlite320_mobilenet_v3_large not available in this torchvision installation.\\n\"\n",
        "                              \"Please upgrade torchvision or adjust the import.\")\n",
        "\n",
        "        if SSDLite320_MobileNet_V3_Large_Weights is None:\n",
        "            # fallback: no weights enum available -> create model without pretrained weights\n",
        "            self._weights = None\n",
        "            self._transforms = T.Compose([T.ToTensor()])\n",
        "            self._model = ssdlite320_mobilenet_v3_large(weights=None).to(self._device)\n",
        "        else:\n",
        "            self._weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT\n",
        "            self._transforms = self._weights.transforms()\n",
        "            self._model = ssdlite320_mobilenet_v3_large(weights=self._weights).to(self._device)\n",
        "\n",
        "        # Head Replacement (SSDLite Head 교체, in_channels 자동 추출)\n",
        "        # SSDLite의 Head는 ModuleList로 되어 있어 내부에 Conv2dNormActivation 등의 블록이 있음.\n",
        "        # 다양한 torchvision 버전에 대해 안전하게 in_channels를 추출하도록 처리.\n",
        "        def _extract_in_channels_from_block(block):\n",
        "            \"\"\"Safely extract in_channels from a block (Sequential with Conv2d or Conv2dNormActivation).\"\"\"\n",
        "            try:\n",
        "                # block 구조: Sequential(Conv2dNormActivation(...), Conv2d(...))\n",
        "                # 또는 Sequential(Conv2d(...), ...)\n",
        "                for module in block:\n",
        "                    if isinstance(module, torch.nn.Conv2d):\n",
        "                        return int(module.in_channels)\n",
        "                    elif hasattr(module, '__iter__'):\n",
        "                        # Nested Sequential (e.g., Conv2dNormActivation)\n",
        "                        for sub_module in module:\n",
        "                            if isinstance(sub_module, torch.nn.Conv2d):\n",
        "                                return int(sub_module.in_channels)\n",
        "                # Fallback: check weight shape\n",
        "                if hasattr(block, 'weight'):\n",
        "                    return int(block.weight.shape[1])\n",
        "                return None\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "        module_list = getattr(self._model.head.classification_head, 'module_list', None)\n",
        "        if module_list is not None and len(module_list) > 0:\n",
        "            in_channels = []\n",
        "            for m in module_list:\n",
        "                ch = _extract_in_channels_from_block(m)\n",
        "                if ch is None:\n",
        "                    # Fallback 1: try out_channels attribute\n",
        "                    ch = getattr(m, 'out_channels', None)\n",
        "                if ch is None:\n",
        "                    # Fallback 2: try to infer from first Conv2d in module\n",
        "                    try:\n",
        "                        if hasattr(m, '__iter__'):\n",
        "                            for sub_m in m:\n",
        "                                if hasattr(sub_m, 'out_channels'):\n",
        "                                    ch = sub_m.out_channels\n",
        "                                    break\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if ch is None:\n",
        "                    # Fallback 3: default MobileNetV3 output channel\n",
        "                    ch = 960\n",
        "                in_channels.append(ch)\n",
        "        else:\n",
        "            # No module_list or empty: use anchor generator based defaults\n",
        "            num_anchors = self._model.anchor_generator.num_anchors_per_location()\n",
        "            in_channels = [960] * len(num_anchors)  # MobileNetV3 Large output channel approximation\n",
        "\n",
        "        num_anchors = self._model.anchor_generator.num_anchors_per_location()\n",
        "        # SSDLite Head를 일반 SSDHead로 교체 (간편한 전이 학습)\n",
        "        self._model.head.classification_head = SSDClassificationHead(in_channels, num_anchors, self._num_classes).to(self._device)\n",
        "        self._model.head.bbox_regression_head = SSDClassificationHead(in_channels, num_anchors, 4).to(self._device)\n",
        "\n",
        "    def getMyName(self): return \"SSDLiteMobileNetV3Transfer\"\n",
        "\n",
        "    def getOptimizer(self):\n",
        "        if self._gubun == \"partial\":\n",
        "            params = [{\"params\": self._model.backbone.parameters(), \"lr\": self._lr * self._backbone_lr_ratio},\n",
        "                      {\"params\": self._model.head.parameters(), \"lr\": self._lr}]\n",
        "        elif self._gubun == \"freeze\":\n",
        "            for param in self._model.backbone.parameters(): param.requires_grad = False\n",
        "            params = self._model.head.parameters()\n",
        "        else: params = self._model.parameters()\n",
        "\n",
        "        optimizer = torch.optim.SGD(params, lr=self._lr, momentum=0.9, weight_decay=5e-4)\n",
        "        scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "        return optimizer, scheduler\n",
        "\n",
        "Lines(\"Define Transfer Models.\")    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ════════════════════════════════════════\n",
        "# ▣ 실행 테스트 함수.\n",
        "# ════════════════════════════════════════\n",
        "def MakeModel(class_name, meta=None, gubun=\"partial\", epochs=5, lr=0.001, backbone_lr_ratio=0.1):\n",
        "    \"\"\"모델 클래스 이름을 받아 해당하는 모델 인스턴스를 생성합니다.\"\"\"\n",
        "    match class_name:\n",
        "        case \"SSD\": return SSD300VGG16Transfer(meta, gubun, epochs, lr, backbone_lr_ratio)\n",
        "        case \"FasterRCNN\": return FasterRCNNResNet50Transfer(meta, gubun, epochs, lr, backbone_lr_ratio)\n",
        "        case \"RetinaNet\": return RetinaNetResNet50Transfer(meta, gubun, epochs, lr, backbone_lr_ratio)\n",
        "        case \"SSDLite\": return SSDLiteMobileNetV3Transfer(meta, gubun, epochs, lr, backbone_lr_ratio)\n",
        "        case _: raise ValueError(f\"Unknown model class name: {class_name}\")\n",
        "\n",
        "def Execute_Training(model_name=\"SSD\", gubun =\"partial\", epochs =5,lr=0.001,backbone_lr_ratio=0.1, batchSize=8):\n",
        "    Lines(f\"Execute_Training Start for {model_name}\")\n",
        "    \n",
        "    # 1. 메타 정보 로드\n",
        "    meta = MyMeta()\n",
        "    if meta.df_trainval is None:\n",
        "        Lines(\"데이터 파일 로드 실패. 실행을 중단합니다.\")\n",
        "        return\n",
        "    \n",
        "    # 2. 모델 생성 (모델의 최적 Transform을 얻기 위해 먼저 생성)\n",
        "    model = MakeModel(model_name, meta=meta, gubun=gubun, epochs=epochs, lr=lr, backbone_lr_ratio=backbone_lr_ratio)\n",
        "    Lines(f\"Model created: {model.getMyName()}\")\n",
        "    \n",
        "    # 3. 모델의 최적 Transform 가져오기\n",
        "    model_transform = model.get_default_transforms()\n",
        "    \n",
        "    # 4. 데이터 분할\n",
        "    train_list, val_list = GetTrainValidationSplit(meta.df_trainval, test_size=0.3, random_state=42)\n",
        "    test_list = meta.test_list\n",
        "    \n",
        "    # 5. 데이터 로더 생성 및 XML 필터링 (GetLoader 내부)\n",
        "    train_loader, val_loader, test_loader = GetLoader(\n",
        "        meta=meta,\n",
        "        train_list=train_list,\n",
        "        val_list=val_list,\n",
        "        test_list=test_list,\n",
        "        transform=model_transform, # 모델별 Transform 전달\n",
        "        batchSize=batchSize\n",
        "    )\n",
        "    \n",
        "    # 6. 모델 학습\n",
        "    model.train(train_loader, val_loader,test_loader)\n",
        "    Lines(\"Execute_Training End\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ════════════════════════════════════════\n",
        "# ▣ 학습 (모델별 권장 옵션)\n",
        "# ════════════════════════════════════════\n",
        "\n",
        "\n",
        "# 1. SSD300 VGG16 (균형잡힌 정확도/속도) : 기본.\n",
        "# 권장: lr=0.001, epochs=10, backbone_lr_ratio=1.0 (전체 학습)\n",
        "Execute_Training(model_name=\"SSD\", gubun=\"partial\", epochs=30, lr=0.001, backbone_lr_ratio=1.0, batchSize=8)\n",
        "\n",
        "# 2. SSDLite MobileNetV3 (Mobile-friendly, 빠름)\n",
        "# 권장: lr=0.001 (높은 학습률), epochs=10, backbone_lr_ratio=0.1 (전이학습)\n",
        "Execute_Training(model_name=\"SSDLite\", gubun=\"partial\", epochs=30, lr=0.001, backbone_lr_ratio=0.1, batchSize=16)\n",
        "\n",
        "# 3. RetinaNet ResNet50 (균형, 중간 속도)\n",
        "# 권장: lr=0.001, epochs=5, gubun=\"freeze\" (backbone 동결)\n",
        "Execute_Training(model_name=\"RetinaNet\", gubun=\"freeze\", epochs=20, lr=0.001, backbone_lr_ratio=0.1, batchSize=8)\n",
        "\n",
        "# 4. Faster R-CNN ResNet50 (높은 정확도, 느림)\n",
        "# 권장: lr=0.005, epochs=5, backbone_lr_ratio=0.1 (가벼운 전이학습)\n",
        "Execute_Training(model_name=\"FasterRCNN\", gubun=\"partial\", epochs=20, lr=0.005, backbone_lr_ratio=0.1, batchSize=4)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
