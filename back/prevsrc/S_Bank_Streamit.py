import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
# from sklearn.model_selection import cross_val_score # Streamlitì—ì„œ ì§ì ‘ ì‚¬ìš©í•˜ê¸°ì—” ë¶€ì í•©

# streamlit run D:\01.project\ì½”ë“œì‡\src\S_Bank_Streamit.py
# ----------------------------------------
# ì›ë³¸ ì½”ë“œì˜ í—¬í¼ í•¨ìˆ˜ ë° ì „ì²˜ë¦¬ ë¡œì§
# (Lines, min_max_clean, age_by_5year_bands)
# ----------------------------------------

# í•œê¸€ í°íŠ¸ ì„¤ì • (Streamlit í´ë¼ìš°ë“œ ë°°í¬ ì‹œ ë³„ë„ ì„¤ì • í•„ìš”)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('fivethirtyeight')

def Lines():
    # Streamlitì—ì„œëŠ” st.divider()ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŠµë‹ˆë‹¤.
    st.divider()

# ì „ì—­ ë§¤í•‘ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
clean_dict = {}
min_max_dict = {}

def min_max_clean(df, col_name, range_count=10, add_name="_clean"):
    global min_max_dict
    new_col_name = col_name + "_" + str(range_count) + add_name
    if(new_col_name not in min_max_dict):
        scaler = MinMaxScaler()
        scaler.fit(df[[col_name]])
        min_max_dict[new_col_name] = scaler
        # st.write(f"âœ… '{col_name}'ì˜ min/maxê°€ í•™ìŠµë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.") # Streamlitì—ì„œëŠ” print ëŒ€ì‹  st.write
    else:
        scaler = min_max_dict[new_col_name]
    
    min_val = scaler.data_min_[0]
    max_val = scaler.data_max_[0]
    bin_width = (max_val - min_val) / range_count
    bins = [min_val + i * bin_width for i in range(range_count)] + [max_val + 1e-5]

    cut_series = pd.cut( 
        df[col_name], 
        bins=bins, 
        include_lowest=True,
        duplicates='drop'
    )
    df[new_col_name] = cut_series.cat.codes
    return df

def age_by_5year_bands(df: pd.DataFrame, source_col: str, target_col: str):
    bins = [1, 10] + list(range(15, 95, 5)) + [np.inf]
    labels = list(range(len(bins) - 1))
    
    df[target_col] = pd.cut(
        df[source_col],
        bins=bins,
        labels=labels,
        right=True,
        include_lowest=True
    ).astype(int)
    
    intervals = pd.cut(
        df[source_col],
        bins=bins,
        right=True,
        include_lowest=True
    ).cat.categories.astype(str)
    
    clean_dict[target_col] = {i: interval for i, interval in enumerate(intervals)}
    return df

# [ìˆ˜ì •ë¨] 'job'ë³„ í‰ê·  ë‚˜ì´ë¥¼ ì €ì¥í•˜ê³  ì¬ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
def preprocess_data(df):
    df = df.drop_duplicates()
    
    # â–£ pdays 999 íŠ¹ì´ê°’ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€ â–£
    df['pdays_contacted'] = np.where(df['pdays'] == 999, 0, 1)
    df['pdays_actual'] = df['pdays'].replace(999, np.nan)
    
    # 'pdays_actual_median'ì´ ì €ì¥ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ê³„ì‚°
    if 'pdays_actual_median' not in clean_dict:
        clean_dict['pdays_actual_median'] = df['pdays_actual'].median()
        
    median_pdays = clean_dict['pdays_actual_median']
    df['pdays_actual'] = df['pdays_actual'].fillna(median_pdays)
    
    df['pdays_contacted_clean'] = df['pdays_contacted'].astype('category').cat.codes
    if 'pdays_contacted_clean' not in clean_dict:
        clean_dict['pdays_contacted_clean'] = {0: 'No Previous Contact (999)', 1: 'Had Previous Contact'}
    
    min_max_clean(df, 'pdays_actual', range_count=10)
    
    # â–£ contact_freq_ratio ìƒì„± ë° ì²˜ë¦¬ â–£
    df['contact_freq_ratio'] = df['previous'] / (df['campaign'] + 1e-6) # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    min_max_clean(df, 'contact_freq_ratio', range_count=20)
    
    # â–£ recent_contact_flag ìƒì„± ë° ì¸ì½”ë”© â–£
    conditions = [
        (df['pdays'] == 999), 
        (df['pdays'] != 999) & (df['poutcome'] == 'success'),
        (df['pdays'] != 999) & (df['poutcome'] == 'failure')
    ]
    choices = ['NoContact', 'Success', 'Failure']
    df['recent_contact_flag'] = np.select(conditions, choices, default='Other')
    df['recent_contact_flag_clean'] = df['recent_contact_flag'].astype('category').cat.codes
    if 'recent_contact_flag_clean' not in clean_dict:
        clean_dict['recent_contact_flag_clean'] = dict(enumerate(df['recent_contact_flag'].astype('category').cat.categories))

    # â–£ ì¹´í…Œê³ ë¦¬ ë²”ì£¼ ìˆ«ìí˜• ì¸ì½”ë”© â–£
    categorical_cols = [
        'job', 'marital', 'education', 'default', 'housing', 'loan', 
        'contact', 'month', 'day_of_week', 'poutcome'
    ]
    
    month_order = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']
    day_order = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']

    for col in categorical_cols:
        df[col] = df[col].astype(str).str.lower() # ì˜ˆì¸¡ ì‹œ ì…ë ¥ë  ë‹¨ì¼ ë°ì´í„°ë„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê²Œ
        
        if col == 'month':
            present_months = df[col].unique().tolist()
            ordered_categories = [m for m in month_order if m in present_months]
            df[col] = pd.Categorical(df[col], categories=ordered_categories, ordered=True)
        elif col == 'day_of_week':
            df[col] = pd.Categorical(df[col], categories=day_order, ordered=True)
        else:
            df[col] = df[col].astype('category')
            
        df[col + '_clean'] = df[col].cat.codes
        
        if col + '_clean' not in clean_dict:
            clean_dict[col + '_clean'] = dict(enumerate(df[col].cat.categories))
            # ì˜ˆì¸¡ì„ ìœ„í•´ (ë¬¸ìì—´ -> ìˆ«ì) ë§¤í•‘ë„ ì €ì¥
            clean_dict[col + '_map'] = {v: k for k, v in clean_dict[col + '_clean'].items()}


    if 'y' in df.columns:
        target_map = {'no': 0, 'yes': 1}
        df['y_clean'] = df['y'].astype(str).str.lower().map(target_map)
        if 'y_clean' not in clean_dict:
            clean_dict['y_clean'] = {v: k for k, v in target_map.items()}

    # â–£ [ìˆ˜ì •] job_age_mean ì²˜ë¦¬ â–£
    # 1. í›ˆë ¨ ì‹œ 'job_age_mean_map' ê³„ì‚° ë° ì €ì¥
    if 'job_age_mean_map' not in clean_dict:
        clean_dict['job_age_mean_map'] = df.groupby('job')['age'].mean().to_dict()
        
    # 2. ì €ì¥ëœ ë§µì„ ì‚¬ìš©í•˜ì—¬ ë§¤í•‘ (í›ˆë ¨/í…ŒìŠ¤íŠ¸/ì˜ˆì¸¡ ê³µí†µ)
    # job_age_mean = df['job'].map(clean_dict['job_age_mean_map'])
    job_age_mean = df['job'].astype(str).map(clean_dict['job_age_mean_map'])

    
    # 3. ë§µì— ì—†ëŠ” ìƒˆë¡œìš´ jobì´ ì˜ˆì¸¡ì— ë“¤ì–´ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´
    #    ì „ì²´ í‰ê·  ë‚˜ì´ë¡œ ë§µì˜ NaN ê°’ì„ ì±„ì›€
    if 'age_mean_global' not in clean_dict:
         clean_dict['age_mean_global'] = df['age'].mean()
    
    job_age_mean = job_age_mean.fillna(clean_dict['age_mean_global'])
    
    df['job_age_mean_diff'] = df['age'] - job_age_mean
    min_max_clean(df, 'job_age_mean_diff', range_count=20)
    
    # ë‚˜ì´ ë° ê¸°íƒ€ ì—°ì†í˜• ë³€ìˆ˜ ë²”ì£¼í™”
    age_by_5year_bands(df, 'age', 'age_clean')
    min_max_clean(df, 'duration', 100)
    min_max_clean(df, 'euribor3m', 100)
    min_max_clean(df, 'nr.employed', 10)
    min_max_clean(df, 'emp.var.rate', 20)
    min_max_clean(df, 'cons.price.idx', 20)
    min_max_clean(df, 'cons.conf.idx', 20)
    
    return df

# ----------------------------------------
# ì›ë³¸ ì½”ë“œì˜ ëª¨ë¸ ë° í”¼ì²˜ ì •ì˜
# ----------------------------------------
feature = [
    "age_clean", "job_clean", "marital_clean", "education_clean", "default_clean",
    "housing_clean", "loan_clean", "contact_clean", "month_clean", "day_of_week_clean",
    # "duration_100_clean",
      "campaign", "previous", "poutcome_clean",
    "cons.price.idx_20_clean", "cons.conf.idx_20_clean", "euribor3m_100_clean",
    "nr.employed_10_clean", "emp.var.rate_20_clean", "pdays_contacted",
    "pdays_actual_10_clean"
    # "contact_freq_ratio_20_clean", # ì›ë³¸ì—ì„œ ì£¼ì„ ì²˜ë¦¬ë¨
    # "recent_contact_flag_clean",
    # "job_age_mean_diff_20_clean"
]
label = ['y_clean']

# ëª¨ë¸ ì •ì˜
def makeModel(input_dim, h1, h2):
    model = nn.Sequential(
        nn.Linear(input_dim, h1),
        nn.ReLU(),
        nn.Linear(h1, h2),
        nn.ReLU(),
        nn.Linear(h2, 1),
    )
    return model

# ----------------------------------------
# Streamlit ìºì‹±ì„ í™œìš©í•œ í•¨ìˆ˜ ì •ì˜
# ----------------------------------------

# @st.cache_data: ë°ì´í„° ë¡œë”© ë° ì „ì²˜ì²˜ëŸ¼ ê²°ê³¼ê°€ ë°”ë€Œì§€ ì•ŠëŠ” ì‘ì—…ì„ ìºì‹œ
@st.cache_data
def load_and_preprocess_data():
    # ë°ì´í„° ë¡œë“œ
    try:
        BASE_DIR = r"D:\01.project\CodeIt\data\bank"
        bank_data = pd.read_csv(BASE_DIR + '\\bank-additional-full.csv', sep=';')
        bank_data_test = pd.read_csv(BASE_DIR + '\\bank-additional.csv', sep=';')
    except FileNotFoundError:
        st.error("ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return None, None, None, None

    # ì „ì²˜ë¦¬ (ì¤‘ìš”: í›ˆë ¨ ë°ì´í„°ë¡œ ë¨¼ì € ë”•ì…”ë„ˆë¦¬ ì±„ìš°ê¸°)
    bank_data = preprocess_data(bank_data)
    bank_data_test = preprocess_data(bank_data_test)

    # ë°ì´í„° ë¶„í•  (ì›ë³¸ ë¡œì§ ì¡´ì¤‘)
    train_val_df = bank_data[feature + label].reset_index(drop=True)
    test_df = bank_data_test[feature + label].reset_index(drop=True)
    
    train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42)
    
    return train_df, val_df, test_df, feature, label, clean_dict


# @st.cache_data: ìŠ¤ì¼€ì¼ë§ (ë°ì´í„°ì…‹ì— ì˜ì¡´)
@st.cache_data
def scale_data(train_df, val_df, test_df, _feature_cols): # _feature_colsëŠ” ìºì‹œ í‚¤ë¡œ ì‚¬ìš©
    scaler = StandardScaler()
    
    # í›ˆë ¨ ë°ì´í„°ë¡œ fit
    train_df_scaled = train_df.copy()
    val_df_scaled = val_df.copy()
    test_df_scaled = test_df.copy()

    train_df_scaled[_feature_cols] = scaler.fit_transform(train_df[_feature_cols])
    
    # ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” transform
    val_df_scaled[_feature_cols] = scaler.transform(val_df[_feature_cols])
    test_df_scaled[_feature_cols] = scaler.transform(test_df[_feature_cols])
    
    return train_df_scaled, val_df_scaled, test_df_scaled, scaler

# @st.cache_resource: ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬ ë“± ë¦¬ì†ŒìŠ¤ë¥¼ ìºì‹œ
# í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë°”ë€” ë•Œë§ˆë‹¤ ì¬í•™ìŠµí•˜ë„ë¡ ì¸ìë¥¼ ë°›ìŒ
# í…ì„œ ì¸ìˆ˜ë¥¼ ì œê±°í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
# ì£¼ì˜: ì´ í•¨ìˆ˜ëŠ” ë” ì´ìƒ @st.cache_resourceë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
# ëª¨ë¸ì„ ìºì‹œí•˜ë ¤ë©´ ëª¨ë¸ ì €ì¥/ë¡œë“œ ë¡œì§ì„ ì‚¬ìš©í•˜ê±°ë‚˜, ìºì‹œë¥¼ í¬ê¸°í•´ì•¼ í•©ë‹ˆë‹¤.

# Streamlit ìºì‹œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ìºì‹œë¥¼ ì œê±°í•˜ê³  í•¨ìˆ˜ ì´ë¦„ì„ ë³€ê²½í•©ë‹ˆë‹¤.
def run_model_training(X_tensor, y_tensor, lr, epochs, h1, h2, feature_count): 
    model = makeModel(feature_count, h1, h2)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    progress_bar = st.progress(0, text="ëª¨ë¸ í•™ìŠµ ì¤‘...")
    loss_list = []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_tensor) # X_tensor ì‚¬ìš©
        loss = criterion(outputs, y_tensor) # y_tensor ì‚¬ìš©
        # ... (ë‚˜ë¨¸ì§€ í•™ìŠµ ë¡œì§ì€ ë™ì¼) ...
        loss.backward()
        optimizer.step()
        
        loss_list.append(loss.item())

        if epoch % 100 == 0 or epoch == epochs - 1:
            progress_bar.progress((epoch + 1) / epochs, text=f"Epoch {epoch:05d} | Loss: {loss.item():.6f}")

        if loss.item() <= 0.00001:
            progress_bar.progress(1.0, text=f"í•™ìŠµ ì¡°ê¸° ì¢…ë£Œ: Epoch {epoch:05d} | Loss: {loss.item():.6f}")
            break
            
    return model, loss_list

def evaluate_model(model, x_tensor, y_tensor):
    model.eval()
    with torch.no_grad():
        outputs = model(x_tensor)
        # BCEWithLogitsLossë¥¼ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ Sigmoidë¥¼ í†µê³¼ì‹œì¼œ í™•ë¥ ë¡œ ë³€í™˜
        probs = torch.sigmoid(outputs)
        predicted = (probs.numpy() > 0.5).astype(float)
        accuracy = accuracy_score(y_tensor, predicted)
    return accuracy

# ----------------------------------------
# Streamlit ì•± UI êµ¬ì„±
# ----------------------------------------

st.set_page_config(layout="wide")
st.title("ğŸ¦ ì€í–‰ ë§ˆì¼€íŒ… ì˜ˆì¸¡ ëª¨ë¸ (PyTorch & Streamlit)")

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
data_load_state = st.text("ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì¤‘...")
data_tuple = load_and_preprocess_data()
if data_tuple[0] is None:
    st.stop()

train_df, val_df, test_df, feature_cols, label_col , mydict= data_tuple
data_load_state.text("ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì™„ë£Œ!")
clean_dict = mydict

st.header("1. ë°ì´í„° íƒìƒ‰")
if st.checkbox("ì²˜ë¦¬ëœ í›ˆë ¨ ë°ì´í„° ë³´ê¸°"):
    st.dataframe(train_df.head(10))
if st.checkbox("ì „ì²˜ë¦¬ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ë³´ê¸° (clean_dict)"):
    st.json(clean_dict, expanded=False)

Lines()

# 2. ì‚¬ì´ë“œë°”: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
st.sidebar.header("âš™ï¸ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°")
LEARNING_RATE = st.sidebar.slider("í•™ìŠµë¥  (Learning Rate)", 0.0001, 0.01, 0.001, 0.0001, format="%.4f")
EPOCHS = st.sidebar.slider("ì—í¬í¬ (Epochs)", 1000, 20000, 5000, 1000) # ê¸°ë³¸ 20000ì€ ë„ˆë¬´ ê¸º
H1 = st.sidebar.number_input("ì€ë‹‰ì¸µ 1 í¬ê¸° (H1)", 1, 128, len(feature_cols))
H2 = st.sidebar.number_input("ì€ë‹‰ì¸µ 2 í¬ê¸° (H2)", 1, 128, len(feature_cols))

# 3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
st.header("2. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")

if st.button("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘í•˜ê¸°", type="primary"):
    
    with st.spinner("ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë° í…ì„œ ë³€í™˜ ì¤‘..."):
        # 3-1. ìŠ¤ì¼€ì¼ë§ (ë°ì´í„°ì— ì˜ì¡´)
        train_scaled, val_scaled, test_scaled, scaler = scale_data(train_df, val_df, test_df, feature_cols)
        
        # 3-2. í…ì„œ ë³€í™˜
        X_train_tensor = torch.tensor(train_scaled[feature_cols].values, dtype=torch.float32)
        y_train_tensor = torch.tensor(train_scaled[label_col].values, dtype=torch.float32)
        X_val_tensor = torch.tensor(val_scaled[feature_cols].values, dtype=torch.float32)
        y_val_tensor = torch.tensor(val_scaled[label_col].values, dtype=torch.float32)
        X_test_tensor = torch.tensor(test_scaled[feature_cols].values, dtype=torch.float32)
        y_test_tensor = torch.tensor(test_scaled[label_col].values, dtype=torch.float32)

    with st.spinner("ëª¨ë¸ í•™ìŠµ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
        # 3-3. ëª¨ë¸ í•™ìŠµ (í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ì˜ì¡´)
        model, loss_history = run_model_training(X_train_tensor, y_train_tensor, LEARNING_RATE, EPOCHS, H1, H2, len(feature_cols))
        st.success("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

        # 3-4. í•™ìŠµ ê³¡ì„  ì‹œê°í™”
        st.subheader("ğŸ“Š í•™ìŠµ ì†ì‹¤(Loss) ê³¡ì„ ")
        fig, ax = plt.subplots()
        ax.plot(loss_history)
        ax.set_title("Training Loss Curve")
        ax.set_xlabel("Epoch")
        ax.set_ylabel("BCEWithLogitsLoss")
        st.pyplot(fig)

        # 3-5. í‰ê°€
        st.subheader("ğŸ¯ ëª¨ë¸ ì •í™•ë„")
        val_accuracy = evaluate_model(model, X_val_tensor, y_val_tensor)
        test_accuracy = evaluate_model(model, X_test_tensor, y_test_tensor)
        
        col1, col2 = st.columns(2)
        col1.metric("ê²€ì¦ ì„¸íŠ¸ ì •í™•ë„ (Validation Accuracy)", f"{val_accuracy:.4f}")
        col2.metric("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •í™•ë„ (Test Accuracy)", f"{test_accuracy:.4f}")

        # 3-6. ì˜ˆì¸¡ì„ ìœ„í•´ í•™ìŠµëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state['trained_model'] = model
        st.session_state['scaler'] = scaler
        st.session_state['feature_cols'] = feature_cols

Lines()

# 4. ì‹¤ì‹œê°„ ì˜ˆì¸¡
st.header("3. ğŸ§‘â€ğŸ’» ì‹¤ì‹œê°„ ì˜ˆì¸¡")

if ('trained_model' in st.session_state) and ('job_map' in clean_dict): # ì´ ì¡°ê±´ ì¶”ê°€
    st.info("ìœ„ì—ì„œ í•™ìŠµëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
    # ì›ë³¸ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì…ë ¥ í¼ ìƒì„±
    # ì „ì²˜ë¦¬ì— í•„ìš”í•œ *ëª¨ë“ * ì›ë³¸ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.
    with st.form("prediction_form"):
        st.subheader("ê³ ê° ì •ë³´ ì…ë ¥")
        
        # ì…ë ¥ í¸ì˜ë¥¼ ìœ„í•´ 2ë‹¨, 3ë‹¨ ì»¬ëŸ¼ ì‚¬ìš©
        col1, col2, col3 = st.columns(3)
        
        with col1:
            age = st.number_input("ë‚˜ì´ (age)", min_value=17, max_value=100, value=40)
            job = st.selectbox("ì§ì—… (job)", options=clean_dict['job_map'].keys())
            marital = st.selectbox("ê²°í˜¼ ì—¬ë¶€ (marital)", options=clean_dict['marital_map'].keys())
            education = st.selectbox("êµìœ¡ ìˆ˜ì¤€ (education)", options=clean_dict['education_map'].keys())
            default = st.selectbox("ì‹ ìš© ë¶ˆëŸ‰ ì—¬ë¶€ (default)", options=clean_dict['default_map'].keys())

        with col2:
            housing = st.selectbox("ì£¼íƒ ëŒ€ì¶œ (housing)", options=clean_dict['housing_map'].keys())
            loan = st.selectbox("ê°œì¸ ëŒ€ì¶œ (loan)", options=clean_dict['loan_map'].keys())
            contact = st.selectbox("ì—°ë½ ìœ í˜• (contact)", options=clean_dict['contact_map'].keys())
            month = st.selectbox("ë§ˆì§€ë§‰ ì—°ë½ ì›” (month)", options=clean_dict['month_map'].keys())
            day_of_week = st.selectbox("ë§ˆì§€ë§‰ ì—°ë½ ìš”ì¼ (day_of_week)", options=clean_dict['day_of_week_map'].keys())

        with col3:
            duration = st.number_input("ë§ˆì§€ë§‰ ì—°ë½ ì‹œê°„(ì´ˆ) (duration)", min_value=0, value=180)
            campaign = st.number_input("ìº í˜ì¸ ì—°ë½ íšŸìˆ˜ (campaign)", min_value=1, value=2)
            pdays = st.number_input("ì´ì „ ìº í˜ì¸ í›„ ê²½ê³¼ ì¼ (pdays)", min_value=0, value=999) # 999ê°€ ê¸°ë³¸
            previous = st.number_input("ì´ì „ ìº í˜ì¸ ì—°ë½ íšŸìˆ˜ (previous)", min_value=0, value=0)
            poutcome = st.selectbox("ì´ì „ ìº í˜ì¸ ê²°ê³¼ (poutcome)", options=clean_dict['poutcome_map'].keys())
            
        st.subheader("ê²½ì œ ì§€í‘œ ì…ë ¥")
        c1, c2, c3, c4, c5 = st.columns(5)
        with c1:
            emp_var_rate = st.number_input("ê³ ìš© ë³€ë™ë¥  (emp.var.rate)", value=-0.1, format="%.1f")
        with c2:
            cons_price_idx = st.number_input("ì†Œë¹„ì ë¬¼ê°€ì§€ìˆ˜ (cons.price.idx)", value=93.2, format="%.1f")
        with c3:
            cons_conf_idx = st.number_input("ì†Œë¹„ì ì‹ ë¢°ì§€ìˆ˜ (cons.conf.idx)", value=-42.0, format="%.1f")
        with c4:
            euribor3m = st.number_input("ìœ ë¦¬ë³´ 3ê°œì›” (euribor3m)", value=1.313, format="%.3f")
        with c5:
            nr_employed = st.number_input("ê³ ìš©ì ìˆ˜ (nr.employed)", value=5099.1, format="%.1f")

        submitted = st.form_submit_button("ì˜ˆì¸¡í•˜ê¸°")

    if submitted:
        # 1. ì…ë ¥ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        input_data = {
            'age': age, 'job': job, 'marital': marital, 'education': education, 'default': default,
            'housing': housing, 'loan': loan, 'contact': contact, 'month': month, 'day_of_week': day_of_week,
            'duration': duration, 'campaign': campaign, 'pdays': pdays, 'previous': previous, 'poutcome': poutcome,
            'emp.var.rate': emp_var_rate, 'cons.price.idx': cons_price_idx, 'cons.conf.idx': cons_conf_idx,
            'euribor3m': euribor3m, 'nr.employed': nr_employed
        }
        input_df = pd.DataFrame([input_data])
        
        # 2. í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•˜ê²Œ ì „ì²˜ë¦¬
        # (clean_dict, min_max_dictê°€ ì´ë¯¸ ì±„ì›Œì ¸ ìˆìœ¼ë¯€ë¡œ ì¬ì‚¬ìš©ë¨)
        try:
            processed_input_df = preprocess_data(input_df)
            feature_input_df = processed_input_df[st.session_state['feature_cols']]
        
            # ğŸš¨ ë””ë²„ê¹… 1: ìŠ¤ì¼€ì¼ë§ ì „ ê°’ í™•ì¸
            # st.dataframe(feature_input_df) # ë””ë²„ê¹… ì‹œ ì ì‹œ í™œì„±í™”
            
            # 3. ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ë³€í™˜
            scaled_input_data = st.session_state['scaler'].transform(feature_input_df)
            
            # ğŸš¨ ë””ë²„ê¹… 2: ìŠ¤ì¼€ì¼ë§ í›„ ê°’ í™•ì¸
            # st.dataframe(pd.DataFrame(scaled_input_data, columns=st.session_state['feature_cols'])) # ë””ë²„ê¹… ì‹œ ì ì‹œ í™œì„±í™”
            
            # 4. í…ì„œë¡œ ë³€í™˜
            input_tensor = torch.tensor(scaled_input_data, dtype=torch.float32)
            
            # 5. ëª¨ë¸ ì˜ˆì¸¡
            model = st.session_state['trained_model']
            model.eval()
            with torch.no_grad():
                output = model(input_tensor)
                prob = torch.sigmoid(output).item() # í™•ë¥ ë¥ 
                prediction = "ê°€ì… (Yes)" if prob > 0.5 else "ë¯¸ê°€ì… (No)"

            # 6. ê²°ê³¼ í‘œì‹œ
            st.subheader("âœ¨ ì˜ˆì¸¡ ê²°ê³¼")
            if prediction == "ê°€ì… (Yes)":
                st.success(f"**{prediction}** (ê°€ì… í™•ë¥ : {prob:.2%})")
            else:
                st.error(f"**{prediction}** (ê°€ì… í™•ë¥ : {prob:.2%})")
                
            with st.expander("ëª¨ë¸ ì…ë ¥ê°’ ë³´ê¸° (ì „ì²˜ë¦¬ ë° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ)"):
                st.dataframe(pd.DataFrame(scaled_input_data, columns=st.session_state['feature_cols']))
                
        except Exception as e:
            st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.error("ì…ë ¥ê°’ì„ í™•ì¸í•˜ê±°ë‚˜ ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")

else:
    st.warning("ë¨¼ì € 'ëª¨ë¸ í•™ìŠµ ì‹œì‘í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")

